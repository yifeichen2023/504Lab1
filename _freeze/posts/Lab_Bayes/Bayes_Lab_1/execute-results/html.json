{
  "hash": "214b87211024ae3fab96759d127fec30",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayes and Penguins\"\nformat: html\neditor: visual\n---\n\n\n\nHere is a worksheet and assignment that combines Bayes (brms) with tidyverse tools. The focus is on the essentials when it comes to simple linear regression with brms.\n\nPlease read and run through this worksheet and answer the conceptual questions that are interleaved within them. At the end of each part, is a coding exercise based on the material you've read until then.\n\n# Part 1: EDA, OLS, BRMS\n\n## Packages and data\n\nLoad the primary packages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggside)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 method overwritten by 'ggside':\n  method from   \n  +.gg   ggplot2\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(brms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\nLoading 'brms' package (version 2.22.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\nAttaching package: 'brms'\n\nThe following object is masked from 'package:stats':\n\n    ar\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(palmerpenguins)\n```\n:::\n\n\n\nWe'll use the `penguins` data set from the **palmerpenguins** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(penguins, package = \"palmerpenguins\")\n\n# Any type of looking at data is a part of EDA \nglimpse(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n\n\n:::\n:::\n\n\n\nYou might divide the data set by the three levels of `species`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins %>% \n  count(species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n```\n\n\n:::\n:::\n\n\n\nTo start, we'll make a subset of the data called `chinstrap`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap <- penguins %>% \n  filter(species == \"Chinstrap\")\n\nglimpse(chinstrap)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 68\nColumns: 8\n$ species           <fct> Chinstrap, Chinstrap, Chinstrap, Chinstrap, Chinstra…\n$ island            <fct> Dream, Dream, Dream, Dream, Dream, Dream, Dream, Dre…\n$ bill_length_mm    <dbl> 46.5, 50.0, 51.3, 45.4, 52.7, 45.2, 46.1, 51.3, 46.0…\n$ bill_depth_mm     <dbl> 17.9, 19.5, 19.2, 18.7, 19.8, 17.8, 18.2, 18.2, 18.9…\n$ flipper_length_mm <int> 192, 196, 193, 188, 197, 198, 178, 197, 195, 198, 19…\n$ body_mass_g       <int> 3500, 3900, 3650, 3525, 3725, 3950, 3250, 3750, 4150…\n$ sex               <fct> female, male, male, female, male, female, female, ma…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n\n\n:::\n:::\n\n\n\nWe've done from a full data set with $N = 344$ rows, to a subset with $n = 68$ rows. (\"\\$\" signs hold LaTex snippets)\n\n## More Exploratory data analysis (EDA)\n\nOur focal variables will be `body_mass_g` and `bill_length_mm`. Here they are in a scatter plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap %>% \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nWe can augment the plot with some nice functions from the **ggside** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap %>% \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE) +\n  # from ggside\n  geom_xsidehistogram(bins = 30) +\n  geom_ysidehistogram(bins = 30) +\n  scale_xsidey_continuous(breaks = NULL) +\n  scale_ysidex_continuous(breaks = NULL) +\n  theme(ggside.panel.scale = 0.25)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nIt's a good idea to get a sense of the sample statistics. Here are the means and SD's for the two variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap %>% \n  summarise(body_mass_g_mean = mean(body_mass_g),\n            body_mass_g_sd = sd(body_mass_g),\n            bill_length_mm_mean = mean(bill_length_mm),\n            bill_length_mm_sd = sd(bill_length_mm)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  body_mass_g_mean body_mass_g_sd bill_length_mm_mean bill_length_mm_sd\n             <dbl>          <dbl>               <dbl>             <dbl>\n1            3733.           384.                48.8              3.34\n```\n\n\n:::\n:::\n\n\n\nAnd you know that more efficient way to compute sample statistics for multiple variables is to first convert the data into the long format with `pivot_longer()`. Then you use a `group_by()` line before the main event in `summarise()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap %>% \n  pivot_longer(cols = c(body_mass_g, bill_length_mm)) %>% \n  group_by(name) %>% \n  summarise(mean = mean(value),\n            sd = sd(value),\n            # count the missing data (if any)\n            n_missing = sum(is.na(value))) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  name             mean     sd n_missing\n  <chr>           <dbl>  <dbl>     <int>\n1 bill_length_mm   48.8   3.34         0\n2 body_mass_g    3733.  384.           0\n```\n\n\n:::\n:::\n\n\n\n### Question 1.1: What do the marginal histograms added by ggside tell you about the distribution of body_mass_g and bill_length_mm individually?\n\nIt shows the distribution of body mass and bill length, which includes the min, max and the center of the distribution.\n\n## OLS\n\nWe'll fit the model\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) \n\\end{align}\n$$\n\nwhere `bill_length_mm` is the *dependent* variable or a *response* variable. The sole predictor is `body_mass_g`. Both variables have $i$ subscripts, which indicate they vary across the $i$ rows in the data set. For now, you might think if $i$ as standing for \"index.\" The last term in the first line, $\\epsilon$, is often called the *error*, or *noise* term. In the second line, we see we're making the conventional assumption the \"errors\" are normally distributed around the regression line.\n\nAn alternative and equivalent way to write that equation is\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i,\n\\end{align}\n$$\n\nwhich is meant to convey we are modeling `bill_length_mm` as normally distributed, with a conditional mean. You don't tend to see equations written this way in the OLS paradigm. However, this style of notation will serve us better when we start modeling our data with other distributions.\n\nThis notation grows on you\n\nFitting the model with the base **R** `lm()` function, which uses the OLS algorithm.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit\nfit1.ols <- lm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n# summarize the results\nsummary(fit1.ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = bill_length_mm ~ 1 + body_mass_g, data = chinstrap)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8399 -2.2370  0.3247  1.8385  9.3138 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.217e+01  3.443e+00   9.344 1.07e-13 ***\nbody_mass_g 4.463e-03  9.176e-04   4.863 7.48e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.887 on 66 degrees of freedom\nMultiple R-squared:  0.2638,\tAdjusted R-squared:  0.2527 \nF-statistic: 23.65 on 1 and 66 DF,  p-value: 7.48e-06\n```\n\n\n:::\n:::\n\n\n\nThe point estimates are in scientific notation. We can pull them with the `coef()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(fit1.ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)  body_mass_g \n32.174192865  0.004462694 \n```\n\n\n:::\n:::\n\n\n\nWe can compute fitted values, or predictions, with the `predict()` function. Here's the default behavior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6        7        8 \n47.79362 49.57870 48.46303 47.90519 48.79773 49.80183 46.67795 48.90930 \n       9       10       11       12       13       14       15       16 \n50.69437 48.68616 49.13243 49.02086 48.68616 50.24810 48.12832 50.24810 \n      17       18       19       20       21       22       23       24 \n46.90108 48.68616 47.57049 51.81005 48.23989 47.34735 45.11601 49.13243 \n      25       26       27       28       29       30       31       32 \n46.90108 50.69437 47.34735 49.13243 48.68616 52.47945 46.45481 51.36378 \n      33       34       35       36       37       38       39       40 \n47.12422 50.47124 48.23989 49.57870 49.35556 53.59512 44.22347 52.25632 \n      41       42       43       44       45       46       47       48 \n49.80183 48.46303 48.01676 47.79362 48.57459 52.03318 47.34735 51.36378 \n      49       50       51       52       53       54       55       56 \n46.67795 48.57459 47.01265 49.80183 48.23989 50.24810 47.12422 47.57049 \n      57       58       59       60       61       62       63       64 \n46.67795 50.24810 49.13243 47.90519 49.80183 48.46303 48.46303 50.02497 \n      65       66       67       68 \n47.34735 49.02086 50.47124 49.02086 \n```\n\n\n:::\n:::\n\n\n\nWe get one prediction, one fitted value, for each case in the data set. We can express the uncertainty around those predictions with confidence intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.ols,\n        interval = \"confidence\") %>% \n  # just the top 6\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 47.79362 46.97456 48.61268\n2 49.57870 48.81580 50.34160\n3 48.46303 47.74771 49.17834\n4 47.90519 47.10905 48.70133\n5 48.79773 48.09864 49.49682\n6 49.80183 48.99783 50.60584\n```\n\n\n:::\n:::\n\n\n\nWe might also ask for a standard error for each prediction.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.ols,\n        se.fit = TRUE) %>% \n  data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit    se.fit df residual.scale\n1  47.79362 0.4102359 66       2.886728\n2  49.57870 0.3821060 66       2.886728\n3  48.46303 0.3582736 66       2.886728\n4  47.90519 0.3987564 66       2.886728\n5  48.79773 0.3501459 66       2.886728\n6  49.80183 0.4026961 66       2.886728\n7  46.67795 0.5648454 66       2.886728\n8  48.90930 0.3504110 66       2.886728\n9  50.69437 0.5185569 66       2.886728\n10 48.68616 0.3513814 66       2.886728\n11 49.13243 0.3554108 66       2.886728\n12 49.02086 0.3521734 66       2.886728\n13 48.68616 0.3513814 66       2.886728\n14 50.24810 0.4550963 66       2.886728\n15 48.12832 0.3789333 66       2.886728\n16 50.24810 0.4550963 66       2.886728\n17 46.90108 0.5296025 66       2.886728\n18 48.68616 0.3513814 66       2.886728\n19 47.57049 0.4359183 66       2.886728\n20 51.81005 0.7050167 66       2.886728\n21 48.23989 0.3707575 66       2.886728\n22 47.34735 0.4647215 66       2.886728\n23 45.11601 0.8407923 66       2.886728\n24 49.13243 0.3554108 66       2.886728\n25 46.90108 0.5296025 66       2.886728\n26 50.69437 0.5185569 66       2.886728\n27 47.34735 0.4647215 66       2.886728\n28 49.13243 0.3554108 66       2.886728\n29 48.68616 0.3513814 66       2.886728\n30 52.47945 0.8273195 66       2.886728\n31 46.45481 0.6015246 66       2.886728\n32 51.36378 0.6270243 66       2.886728\n33 47.12422 0.4961023 66       2.886728\n34 50.47124 0.4856973 66       2.886728\n35 48.23989 0.3707575 66       2.886728\n36 49.57870 0.3821060 66       2.886728\n37 49.35556 0.3661365 66       2.886728\n38 53.59512 1.0397147 66       2.886728\n39 44.22347 1.0105441 66       2.886728\n40 52.25632 0.7859885 66       2.886728\n41 49.80183 0.4026961 66       2.886728\n42 48.46303 0.3582736 66       2.886728\n43 48.01676 0.3882941 66       2.886728\n44 47.79362 0.4102359 66       2.886728\n45 48.57459 0.3541019 66       2.886728\n46 52.03318 0.7451900 66       2.886728\n47 47.34735 0.4647215 66       2.886728\n48 51.36378 0.6270243 66       2.886728\n49 46.67795 0.5648454 66       2.886728\n50 48.57459 0.3541019 66       2.886728\n51 47.01265 0.5126128 66       2.886728\n52 49.80183 0.4026961 66       2.886728\n53 48.23989 0.3707575 66       2.886728\n54 50.24810 0.4550963 66       2.886728\n55 47.12422 0.4961023 66       2.886728\n56 47.57049 0.4359183 66       2.886728\n57 46.67795 0.5648454 66       2.886728\n58 50.24810 0.4550963 66       2.886728\n59 49.13243 0.3554108 66       2.886728\n60 47.90519 0.3987564 66       2.886728\n61 49.80183 0.4026961 66       2.886728\n62 48.46303 0.3582736 66       2.886728\n63 48.46303 0.3582736 66       2.886728\n64 50.02497 0.4272392 66       2.886728\n65 47.34735 0.4647215 66       2.886728\n66 49.02086 0.3521734 66       2.886728\n67 50.47124 0.4856973 66       2.886728\n68 49.02086 0.3521734 66       2.886728\n```\n\n\n:::\n:::\n\n\n\nInstead of relying on predictions from the values in the data, we might instead define a sequence of values from the predictor variable. We'll call those `nd`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnd <- tibble(body_mass_g = seq(from = min(chinstrap$body_mass_g),\n                               to = max(chinstrap$body_mass_g),\n                               length.out = 50))\n\nglimpse(nd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 50\nColumns: 1\n$ body_mass_g <dbl> 2700.000, 2742.857, 2785.714, 2828.571, 2871.429, 2914.286…\n```\n\n\n:::\n:::\n\n\n\nWe can insert our `nd` data into the `newdata` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.ols,\n        interval = \"confidence\",\n        newdata = nd) %>% \n  # just the top 6\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 44.22347 42.20585 46.24108\n2 44.41473 42.47057 46.35888\n3 44.60598 42.73489 46.47708\n4 44.79724 42.99874 46.59574\n5 44.98850 43.26207 46.71493\n6 45.17976 43.52482 46.83469\n```\n\n\n:::\n:::\n\n\n\nNow we wrangle those predictions a bit and pump the results right into `ggplot()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.ols,\n        interval = \"confidence\",\n        newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% confidence interval ribbon\n  geom_ribbon(aes(ymin = lwr, ymax = upr),\n              alpha = 1/3) +\n  # point estimate line\n  geom_line(aes(y = fit)) +\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nIf we wanted to, we could look at the residuals with help from the `residuals()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals(fit1.ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1          2          3          4          5          6          7 \n-1.2936220  0.4213003  2.8369738 -2.5051894  3.9022718 -4.6018344 -0.5779485 \n         8          9         10         11         12         13         14 \n 2.3907044 -4.6943732  2.6138391 -2.5324303  2.6791371 -1.6861609  1.7518962 \n        15         16         17         18         19         20         21 \n-2.2283241  0.2518962  3.3989168  9.3138391 -1.1704873 -2.6100467 -5.8398915 \n        22         23         24         25         26         27         28 \n 1.1526474 -1.9160056  1.4675697 -0.2010832  1.3056268  3.1526474  0.3675697 \n        29         30         31         32         33         34         35 \n-2.2861609  0.3205492 -5.5548138  2.8362227 -4.6242179  0.5287615  1.4601085 \n        36         37         38         39         40         41         42 \n-2.0786997 -1.7555650 -1.5951243  2.6765332  1.2436839 -0.8018344 -2.2630262 \n        43         44         45         46         47         48         49 \n 2.8832432 -2.2936220  2.3254065 -1.2331814  2.7526474 -2.3637773  4.8220515 \n        50         51         52         53         54         55         56 \n 1.2254065  1.0873494  1.5981656 -2.5398915  0.4518962 -4.6242179  4.6295127 \n        57         58         59         60         61         62         63 \n-1.4779485 -0.9481038  1.0675697 -2.3051894  2.0981656 -1.6630262 -2.7630262 \n        64         65         66         67         68 \n 5.7750309 -3.8473526  0.5791371  0.3287615  1.1791371 \n```\n\n\n:::\n:::\n\n\n\nHere we might put them in a tibble and display them in a plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# put them in a tibble\ntibble(r = residuals(fit1.ols)) %>% \n  # plot!\n  ggplot(aes(x = r)) +\n  geom_histogram(binwidth = 1)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n### Question 1.2: Can you predict what the mean value, and standard deviations will be? Why? Calculate it. Compare this against outputs in summary(fit1.ols) and explain. Map the values you find to the latex equations before.\n\nBy math, the mean of residuals should be 0 (property of OLS). By looking at the histogram of residuals, it should be around 2. Outpus from summary(fit1.ols) says the SD of residuals is around 2.9, which is the sigma_epsilon from the first equation, residuals/epsilon_i \\~ Normal(0, sigma_epsilon).\n\n## Bayes with default settings\n\nWe'll be fitting our Bayesian models with the **brms** package. The primary function is `brm()`.\n\n`brm()` can work a lot like the OLS-based `lm()` function. For example, here's how to fit a Bayesian version of our OLS model `fit1.ols`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\nNotice what's happening in the console, below. We'll get into the details of what just happened later. For now, appreciate we just fit our first Bayesian model, and it wasn't all that hard.\n\nSummarize the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.23      3.51    25.33    39.15 1.00     4903     2930\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4951     3097\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.94      0.27     2.46     3.54 1.00     1556     1449\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n### Question 1.3: Contrast the language of in the `brm()` output from the in the `lm()` output. Ignore 'Rhat,' 'Bulk_ESS,' and 'Tail_ESS' for now.\n\nlm treats the parameters as fixed unknowns and compares those against zero, wherer as brm treats the parameters as a distribution of a random var.\n\nWe can get a quick and dirty plot of the fitted line with the `conditional_effects()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(fit1.b)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# %>% \n#   plot(points = TRUE)\n```\n:::\n\n\n\n## Coefficients and coefficient plots\n\nWe might want to compare the coefficient summaries from the OLS model to those from the Bayesian model. Here's the frequentist summary:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(coef(fit1.ols),              # point estimates\n      sqrt(diag(vcov(fit1.ols))),  # standard errors\n      confint(fit1.ols))           # 95% CIs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                             2.5 %       97.5 %\n(Intercept) 32.174192865 3.4433622902 25.299298235 39.049087495\nbody_mass_g  0.004462694 0.0009176106  0.002630625  0.006294763\n```\n\n\n:::\n:::\n\n\n\nWe can compute a focused summary of the Bayesian model with the `fixef()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.225591477 3.5147307595 25.325973703 39.145581887\nbody_mass_g  0.004454168 0.0009363392  0.002626482  0.006296705\n```\n\n\n:::\n:::\n\n\n\nIn this case, the results are very similar.\n\nWe can also pull this information from our OLS model with the `broom::tidy()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(fit1.ols, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept) 32.2      3.44          9.34 1.07e-13 25.3      39.0    \n2 body_mass_g  0.00446  0.000918      4.86 7.48e- 6  0.00263   0.00629\n```\n\n\n:::\n:::\n\n\n\nIf you would like to use the `tidy()` function with your **brms** models, it will have to be the version of `tidy()` from the **broom.mixed** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(fit1.b)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in tidy.brmsfit(fit1.b): some parameter names contain underscores: term\nnaming may be unreliable!\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  effect   component group    term         estimate std.error conf.low conf.high\n  <chr>    <chr>     <chr>    <chr>           <dbl>     <dbl>    <dbl>     <dbl>\n1 fixed    cond      <NA>     (Intercept)  32.2      3.51     25.3      39.1    \n2 fixed    cond      <NA>     body_mass_g   0.00445  0.000936  0.00263   0.00630\n3 ran_pars cond      Residual sd__Observa…  2.94     0.273     2.46      3.54   \n```\n\n\n:::\n:::\n\n\n\nHere's how to wrangle and combine these two results into a single data frame. Then we'll make a coefficient plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(\n  tidy(fit1.ols, conf.int = TRUE) %>% select(term, estimate, contains(\"conf\")),\n  tidy(fit1.b) %>% select(term, estimate, contains(\"conf\")) %>% filter(term != \"sd__Observation\")\n) %>% \n  mutate(method = rep(c(\"lm()\", \"brm()\"), each = 2)) %>% \n  \n  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = method)) +\n  geom_pointrange() +\n  scale_x_continuous(\"parameter space\", expand = expansion(mult = 0.2)) +\n  scale_y_discrete(expand = expansion(mult = 5)) +\n  facet_wrap(~ term, scales = \"free_x\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\nAt a superficial level for simple conventional regression type models, the results from a Bayesian `brm()` model will be very similar to those from an OLS `lm()` model. This will not always be case, and even in this example there are many differences once we look below the surface.\n\n## More Questions/Exercise\n\nGo back to the full `penguins` data set. This time, make a subset of the data called `gentoo`, which is only the cases for which `species == \"Gentoo\"`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngentoo <- penguins %>% \n  filter(species == \"Gentoo\")\ngentoo <- na.omit(gentoo)\n                  \ngentoo_ols <- lm(\n  data = gentoo,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n# Create prediction data\nnd_gentoo <- tibble(body_mass_g = seq(from = min(gentoo$body_mass_g),\n                               to = max(gentoo$body_mass_g),\n                               length.out = 50))\n\n# Plot OLS predictions\npredict(gentoo_ols,\n        interval = \"confidence\",\n        newdata = nd_gentoo) %>% \n  data.frame() %>% \n  bind_cols(nd_gentoo) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr),\n              alpha = 1/3) +\n  geom_line(aes(y = fit)) +\n  geom_point(data = gentoo,\n             aes(y = bill_length_mm)) \n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngentoo_b <- brm(\n  data = gentoo,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 14.0.3 (clang-1403.0.22.14.1)’\nusing SDK: ‘MacOSX13.3.sdk’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include <cmath>\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 1:                0.023 seconds (Sampling)\nChain 1:                0.06 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.036 seconds (Warm-up)\nChain 2:                0.024 seconds (Sampling)\nChain 2:                0.06 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 3:                0.026 seconds (Sampling)\nChain 3:                0.06 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 4:                0.026 seconds (Sampling)\nChain 4:                0.063 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nconditional_effects(gentoo_b) %>%\n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(\n  tidy(gentoo_ols, conf.int = TRUE) %>% select(term, estimate, contains(\"conf\")),\n  tidy(gentoo_b) %>% select(term, estimate, contains(\"conf\")) %>% filter(term != \"sd__Observation\")\n) %>% \n  mutate(method = rep(c(\"lm()\", \"brm()\"), each = 2)) %>% \n  \n  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = method)) +\n  geom_pointrange() +\n  scale_x_continuous(\"parameter space\", expand = expansion(mult = 0.2)) +\n  scale_y_discrete(expand = expansion(mult = 5)) +\n  facet_wrap(~ term, scales = \"free_x\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\nCan you fit the same OLS model to these data?\n\nYes, after excluding NA\n\nHow about plotting the results with `predict()`?\n\nYes\n\nCan you fit the same default Bayesian `brm()` model to these data?\n\nYes\n\nHow about plotting the results with `conditional_effects()`?\n\nYes\n\nCan you make a coefficient plot comparing the new OLS and Bayesian beta coefficients?\n\nYes\n\n# Part 2: Penguins and their posteriors\n\n## Exploring model results\n\nWe can extract the posterior draws from our Bayesian models with the `as_draws_df()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A draws_df: 1000 iterations, 4 chains, and 6 variables\n   b_Intercept b_body_mass_g sigma Intercept lprior lp__\n1           34        0.0040   3.1        49   -4.3 -171\n2           32        0.0046   3.1        49   -4.3 -172\n3           28        0.0055   2.9        48   -4.3 -172\n4           31        0.0048   2.8        48   -4.3 -172\n5           37        0.0033   3.0        49   -4.3 -172\n6           27        0.0058   2.7        49   -4.2 -173\n7           31        0.0047   2.8        49   -4.3 -171\n8           28        0.0056   2.8        49   -4.3 -172\n9           39        0.0026   3.1        49   -4.3 -173\n10          38        0.0028   3.1        49   -4.3 -173\n# ... with 3990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n```\n\n\n:::\n:::\n\n\n\nNote the meta data. We can get a sense of the full posterior distributions of the $\\beta$ parameters with plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# wrangle\nas_draws_df(fit1.b) %>% \n  pivot_longer(starts_with(\"b_\")) %>% \n  \n  # plot!\n  ggplot(aes(x = value)) + \n  # geom_density(fill = \"grey20\") +\n  geom_histogram(bins = 40) +\n  facet_wrap(~ name, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n\nWe might summarize those posterior distributions with basic descriptive statistics, like their means, SD's, and inner 95-percentile range.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit1.b) %>% \n  pivot_longer(starts_with(\"b_\")) %>% \n  group_by(name) %>% \n  summarise(mean = mean(value),\n            sd = sd(value),\n            ll = quantile(value, probs = 0.025),\n            ul = quantile(value, probs = 0.975))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  name              mean       sd       ll       ul\n  <chr>            <dbl>    <dbl>    <dbl>    <dbl>\n1 b_Intercept   32.2     3.51     25.3     39.1    \n2 b_body_mass_g  0.00445 0.000936  0.00263  0.00630\n```\n\n\n:::\n:::\n\n\n\nNotice how these values match up exactly with those from `fixef()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.225591477 3.5147307595 25.325973703 39.145581887\nbody_mass_g  0.004454168 0.0009363392  0.002626482  0.006296705\n```\n\n\n:::\n:::\n\n\n\nThus,\n\n-   The Bayesian posterior mean is analogous to the frequentist point estimate.\n-   The Bayesian posterior SD is analogous to the frequentist standard error.\n-   The Bayesian posterior percentile-based 95% (credible) interval is analogous to the frequentist 95% confidence interval.\n\nThese are not exactly the same, mind you. But they serve similar functions.\n\nWe can also get a sense of these distributions with the `plot()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit1.b)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\nIgnore the trace plots on the right for a moment. And let's consider the `pairs()` plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(fit1.b)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# we can adjust some of the settings with the off_diag_args argument\npairs(fit1.b, off_diag_args = list(size = 1/4, alpha = 1/4))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-34-2.png){width=672}\n:::\n:::\n\n\n\n### Question 2.1 : In the parlance of Probability, do you know what is the term by which the distributions in the diagonal of the above plot are known as? And the distributions in the off-diagonal?\n\nIn the diagonal: marginal distributions. Probability dist of each parameter. Off-diagonal: joint distribution, how another parameter value may affect current parameter's value, showing the dependecies between parameters.\n\nNotice how the two $\\beta$ parameters seem to have a strong negative correlation. We can quantify that correlation with the `vcov()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvcov(fit1.b)                      # variance/covariance metric\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Intercept   body_mass_g\nIntercept   12.353332312 -3.274187e-03\nbody_mass_g -0.003274187  8.767310e-07\n```\n\n\n:::\n\n```{.r .cell-code}\nvcov(fit1.b, correlation = TRUE)  # correlation metric\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Intercept body_mass_g\nIntercept    1.0000000  -0.9948973\nbody_mass_g -0.9948973   1.0000000\n```\n\n\n:::\n:::\n\n\n\nThis correlation/covariance among the parameters is not unique to Bayesian models. Here's the `vcov()` output for the OLS model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvcov(fit1.ols)  # variance/covariance metric\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             (Intercept)   body_mass_g\n(Intercept) 11.856743861 -3.143295e-03\nbody_mass_g -0.003143295  8.420092e-07\n```\n\n\n:::\n:::\n\n\n\nI'm not aware of an easy way to get that output in a correlation metric for our OLS model. Here's how to compute the correlation by hand.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncov_xy <- vcov(fit1.ols)[2, 1]  # covariance between the intercept and slope\nvar_x  <- vcov(fit1.ols)[1, 1]  # variance for the intercept\nvar_y  <- vcov(fit1.ols)[2, 2]  # variance for the slope\n\n# convert the covariance into a correlation\ncov_xy / (sqrt(var_x) * sqrt(var_y))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.9948188\n```\n\n\n:::\n:::\n\n\n\nThat code follows the definition of a covariance, which can be expressed as\n\n$$\n\\text{Cov}(x, y) = \\rho \\sigma_x \\sigma_y,\n$$\n\nwhere $\\sigma_x$ is the standard deviation for x, $\\sigma_y$ is the standard deviation for y, and $\\rho$ is their correlation. And thus, you can convert a covariance into a correlation with the formula\n\n$$\n\\rho = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y},\n$$\n\nwhere $\\sigma_{xy}$ is the covariance of x and y.\n\n## Draws\n\nLet's save the `as_draws_df()` output for our model as an object called `draws`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- as_draws_df(fit1.b)\nglimpse(draws)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,000\nColumns: 9\n$ b_Intercept   <dbl> 33.63574, 32.09239, 28.07307, 30.52911, 36.80700, 27.361…\n$ b_body_mass_g <dbl> 0.004047507, 0.004575268, 0.005454389, 0.004797569, 0.00…\n$ sigma         <dbl> 3.080713, 3.128354, 2.871590, 2.820956, 2.992221, 2.7235…\n$ Intercept     <dbl> 48.74544, 49.17227, 48.43479, 48.43886, 48.95258, 49.115…\n$ lprior        <dbl> -4.336407, -4.325042, -4.312722, -4.300020, -4.300342, -…\n$ lp__          <dbl> -171.3029, -171.7304, -172.0721, -171.5649, -171.7970, -…\n$ .chain        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ .iteration    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ .draw         <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n```\n\n\n:::\n:::\n\n\n\nFor each parameter in the model, we have 4,000 draws from the posterior.\n\n### Question 2.2: How does this concept relate to representing uncertainty? Can you anticipate how predictions are made based upon these 4000 draws and the linear regression formula?\n\nThe 4000 draws represents the uncertainty we have about the model parameters. It gave us a distribution of possible parameter values rather than a single estimate. For each body mass value, we will compute the bill length for 4000 times using each of the draw to make prediction of the bill length.\n\n$$\\widehat{\\text{bill_length_mm}}_i = \\beta_0 + \\beta_1 \\text{body_mass_g}_i.$$\n\nLet's break the 4000 draws down with our `draws` object.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# adjust the parameter names \ndraws <- draws %>% \n  mutate(beta0 = b_Intercept,\n         beta1 = b_body_mass_g)\n\n# Note: go through this one line at a time\ndraws %>% \n  select(.draw, beta0, beta1) %>% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %>% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %>% \n  \n  ggplot(aes(x = y_hat)) +\n  geom_histogram(bins = 40) +\n  labs(title = \"Bayesians have posterior distributions\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\nHere's what that is for the OLS model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.ols,\n        newdata = tibble(body_mass_g = mean(chinstrap$body_mass_g)),\n        interval = \"confidence\") %>% \n  data.frame() %>% \n  \n  ggplot(aes(x = fit, xmin = lwr, xmax = upr, y = 0)) +\n  geom_pointrange() +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Frequentists have point estmates and 95% CI's\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\nAnother handy way to present a Bayesian posterior is as a density with a point-interval summary below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggdist) #for stat_half_eye and mean_qi\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'ggdist'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n```\n\n\n:::\n\n```{.r .cell-code}\ndraws %>% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %>% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %>% \n  \n  ggplot(aes(x = y_hat)) +\n  stat_halfeye(point_interval = mean_qi, .width = .95) +\n  # scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Bayesians have posterior distributions\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n\nThe dot at the base of the plot is the posterior mean, and the horizontal line marks the 95% percentile-based interval. If you'd like to mark the median instead, set `point_interval = median_qi`. If you're like a different kind of horizontal interval, adjust the `.width` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws %>% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %>% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %>% \n  \n  ggplot(aes(x = y_hat)) +\n  # note the changes to this line\n  stat_halfeye(point_interval = median_qi, .width = c(.5, .99)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Bayesians have posterior distributions\",\n       subtitle = \"The dot marks the median.\\nThe thicker line marks the 50% interval, and\\nthe thinner line marks the 99% interval.\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n\n## About those means, SD's, and intervals.\n\nYou can describe a Bayesian posterior in a lot of different ways. Earlier we said the posterior mean is the Bayesian point estimate. This isn't strictly true. Means are very popular, but you can summarize a posterior by its mean, median, or mode.\n\nLet's see what this looks like in practice. First, we compute and save our statistics for each of our model parameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoints <- draws %>% \n  rename(`beta[0]` = beta0,\n         `beta[1]` = beta1) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  group_by(parameter) %>% \n  summarise(mean = mean(value),\n            median = median(value),\n            mode = Mode(value)) %>% \n  pivot_longer(starts_with(\"m\"), names_to = \"statistic\")\n\n# what?\npoints\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 3\n  parameter statistic    value\n  <chr>     <chr>        <dbl>\n1 beta[0]   mean      32.2    \n2 beta[0]   median    32.2    \n3 beta[0]   mode      32.1    \n4 beta[1]   mean       0.00445\n5 beta[1]   median     0.00446\n6 beta[1]   mode       0.00451\n7 sigma     mean       2.94   \n8 sigma     median     2.91   \n9 sigma     mode       2.88   \n```\n\n\n:::\n:::\n\n\n\nNow plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws %>% \n  rename(`beta[0]` = beta0,\n         `beta[1]` = beta1) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  geom_density() +\n  geom_vline(data = points,\n             aes(xintercept = value, color = statistic),\n             size = 3/4) +\n  scale_color_viridis_d(option = \"A\", end = .8) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, labeller = label_parsed, scales = \"free\", ncol = 1) +\n  theme(strip.text = element_text(size = 14))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\n\n### Question 2.3: Discuss the skew in $\\sigma$.Why it might arise, etc.?\n\nMost likely because of the outliers, the data itself is probably skew to right causing the SD to be more positive sometimes. Additionally, since SD has to be positive, it may naturally just skew to the right.\n\n-   The mean is the **brms** default summary, and McElreath (2015, 2020) defaulted to the mean in his texts.\n-   The median is also available for many **brms** functions, and it's what Gelman et al (2020) recommend.\n-   The mode can be attractive for very skewed distributions, and it's what Kruschke (2015) used in his text.\n\nWith many **brms** functions, you can request the median by setting `robust = TRUE`. For example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(fit1.b)                 # means\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.225591477 3.5147307595 25.325973703 39.145581887\nbody_mass_g  0.004454168 0.0009363392  0.002626482  0.006296705\n```\n\n\n:::\n\n```{.r .cell-code}\nfixef(fit1.b, robust = TRUE)  # medians\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.208500277 3.4772506067 25.325973703 39.145581887\nbody_mass_g  0.004457947 0.0009124042  0.002626482  0.006296705\n```\n\n\n:::\n:::\n\n\n\n### Question 2.4: Given the skew in sigma and what you know about summary statistics, what might be the implication of using just the mean, median, or mode of posteriors to make a prediction?\n\nThe skew in sigma may suggest that using the mean to make a prediction may pull it over to the longer tail. leading to wider prediction interval, compare to median. Mode is not a good measure since it only captures the peak and ignore the rest of the data. Median could be good, but may not capture extreme value.\n\n#### SD's and MAD SD's.\n\nEarlier we said the posterior SD is the Bayesian standard error. This isn't strictly true. You can also use the *median absolute deviation* (MAD SD). If we let $M$ stand for the median of some variable $y$, which varies across $i$ cases, we can define the MAD SD as\n\n$$\\textit{MAD SD} = 1.4826 \\times \\operatorname{median}_{i = 1}^n |y_i - M|,$$\n\nwhere $1.4826$ is a constant that scales the MAD SD into a standard-deviation metric. Here's what this looks like in practice.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# go through this line by line\ndraws %>% \n  select(beta0) %>% \n  mutate(mdn = median(beta0)) %>% \n  mutate(`|yi - mdn|` = abs(beta0 - mdn)) %>% \n  summarise(MAD_SD = 1.4826 * median(`|yi - mdn|`))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  MAD_SD\n   <dbl>\n1   3.48\n```\n\n\n:::\n:::\n\n\n\nBase **R** also has a `mad()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?mad\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHelp on topic 'mad' was found in the following packages:\n\n  Package               Library\n  posterior             /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n  stats                 /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n\n\nUsing the first match ...\n```\n\n\n:::\n\n```{.r .cell-code}\ndraws %>% \n  summarise(MAD_SD = mad(beta0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  MAD_SD\n   <dbl>\n1   3.48\n```\n\n\n:::\n:::\n\n\n\nYou can request the MAD SD from many **brms** functions by setting `robust = TRUE`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(fit1.b)                 # SD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.225591477 3.5147307595 25.325973703 39.145581887\nbody_mass_g  0.004454168 0.0009363392  0.002626482  0.006296705\n```\n\n\n:::\n\n```{.r .cell-code}\nfixef(fit1.b, robust = TRUE)  # MAD SD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.208500277 3.4772506067 25.325973703 39.145581887\nbody_mass_g  0.004457947 0.0009124042  0.002626482  0.006296705\n```\n\n\n:::\n:::\n\n\n\n-   To my eye, many authors (e.g., Kruschke, McElreath) just use the SD.\n-   Gelman et al (see Section 5.3) recommend the MAD SD.\n\n#### Bayesian intervals.\n\nBayesians describe the widths of their posteriors with intervals. I've seen these variously described as confidence intervals, credible intervals, probability intervals, and even uncertainty intervals. My recommendation is just pick a term, and clearly tell your audience what you mean (e.g., at the end of a Method section in a journal article).\n\nTo my eye, the most popular interval is a 95% percentile-based interval. 95% is conventional, perhaps due to the popularity of the 95% frequentist confidence interval, which is related to the 0.05 alpha level used for the conventional $p$-value cutoff. However, you can use other percentiles. Some common alternatives are 99%, 89%, 80%, and 50%.\n\nAlso, Bayesian intervals aren't always percentile based. An alternative is the highest posterior density interval (HPDI), which has mathematical properties some find desirable.\n\n**brms** only supports percentile-based intervals, but it does allow for a variety of different ranges via the `prob` argument. For example, here's how to request 80% intervals in `summary()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit1.b, prob = .80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.23      3.51    27.68    36.69 1.00     4903     2930\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4951     3097\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.94      0.27     2.61     3.29 1.00     1556     1449\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nRegarding interval widths:\n\n-   95% Intervals are widely used.\n-   McElreat likes 89% intervals, and uses them as a default in his **rethinking** package.\n-   Some of the **bayesplot**, **ggdist**, and **tidybayes** functions return 80% intervals.\n-   Some of the **ggdist**, and **tidybayes** functions return 66% or 50% intervals.\n-   I've heard Gelman report his fondness for 50% intervals on his blog (https://statmodeling.stat.columbia.edu/2016/11/05/why-i-prefer-50-to-95-intervals/).\n\nRegarding interval types:\n\n-   Percentile-based intervals are widely used in the Stan ecosystem, and are supported in texts like Gelman et al.\n-   Kruschke has consistently advocates for HPDI's in his articles, and in his text.\n\n### Posterior summaries with **tidybayes**.\n\nMatthew Kay's **tidybayes** package (https://mjskay.github.io/tidybayes/) offers an array of convenience functions for summarizing posterior distributions with points and intervals. See the *Point summaries and intervals* section of Kay's *Extracting and visualizing tidy draws from brms models* vignette (https://mjskay.github.io/tidybayes/articles/tidy-brms.html#point-summaries-and-intervals) for a detailed breakdown. In short, the family of functions use the naming scheme `[median|mean|mode]_[qi|hdi]`. Here are a few examples.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws %>% mean_qi(beta0)                        # mean and 95% percentile interval\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\nWarning: Dropping 'draws_df' class as required metadata was removed.\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  beta0 .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  32.2   25.3   39.1   0.95 mean   qi       \n```\n\n\n:::\n\n```{.r .cell-code}\ndraws %>% median_qi(beta0, .width = .80)        # median and 80% percentile interval\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\nWarning: Dropping 'draws_df' class as required metadata was removed.\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  beta0 .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  32.2   27.7   36.7    0.8 median qi       \n```\n\n\n:::\n\n```{.r .cell-code}\ndraws %>% mode_hdi(beta0, .width = c(.5, .95))  # mode, with 95 and 50% HPDI's\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\nWarning: Dropping 'draws_df' class as required metadata was removed.\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  beta0 .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  32.1   29.8   34.5   0.5  mode   hdi      \n2  32.1   25.6   39.3   0.95 mode   hdi      \n```\n\n\n:::\n:::\n\n\n\nAs an aside, the `Mode()` function we used a while back was also from **tidybayes**.\n\n### Spaghetti plots.\n\nRemember how we said the `draw` was something like 4,000 separate equations for our Bayesian model? Let's see that again.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws %>% \n  select(.draw, beta0, beta1) %>% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %>% \n  # here's the equation\n  mutate(y_hat = beta0 + beta1 * body_mass_g) %>% \n  # subset the top 6\n  head()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  .draw beta0   beta1 body_mass_g y_hat\n  <int> <dbl>   <dbl>       <dbl> <dbl>\n1     1  33.6 0.00405       3733.  48.7\n2     2  32.1 0.00458       3733.  49.2\n3     3  28.1 0.00545       3733.  48.4\n4     4  30.5 0.00480       3733.  48.4\n5     5  36.8 0.00325       3733.  49.0\n6     6  27.4 0.00583       3733.  49.1\n```\n\n\n:::\n:::\n\n\n\nOne way we might emphasize the 4,000 equations is with a spaghetti plot. When we display the fitted line for `bill_length_mm` over the range of `body_mass_g` values, we can display a single line for each posterior draw. Here's what that can look like.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(chinstrap$body_mass_g)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2700 4800\n```\n\n\n:::\n\n```{.r .cell-code}\n# Note: go through this one line at a time\ndraws %>% \n  select(.draw, beta0, beta1) %>% \n  expand_grid(body_mass_g = range(chinstrap$body_mass_g)) %>% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %>% \n  \n  # plot!\n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw)) +\n  geom_line(linewidth = 1/10, alpha = 1/10)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\n\n\nIt might be easier to see what's going on with a random subset of, say, 10 of the posterior draws.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\n\ndraws %>% \n  # take a random sample of 10 rows\n  slice_sample(n = 10) %>% \n  select(.draw, beta0, beta1) %>% \n  expand_grid(body_mass_g = range(chinstrap$body_mass_g)) %>% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %>% \n  \n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw)) +\n  geom_line(linewidth = 1/2, alpha = 1/2)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n:::\n\n\n\nWhile we're at it, let's take 20 draws and do a little color coding.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\n\ndraws %>% \n  # take a random sample of 20 rows\n  slice_sample(n = 20) %>% \n  select(.draw, beta0, beta1) %>% \n  expand_grid(body_mass_g = range(chinstrap$body_mass_g)) %>% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %>% \n  \n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw, color = beta0)) +\n  geom_line() +\n  scale_color_viridis_c(expression(beta[0]~(the~intercept)), end = .9)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\n\nDo you remember how we said $\\beta_0$ and $\\beta_1$ had a strong negative correlation? Notice how the lines computed by lower $\\beta_0$ values also tend to have higher slopes. This will happen all the time with conventional regression models.\n\n### Question 2.5: We have done all this without yet specifying a prior. What do you think is going on?\n\nI am pretty sure the default prior have no strong favor of one parameter value over the other, therefore, we get something very similar as ols, which also have no prior on the parameter values.\n\n## Question/Exercise:\n\nIn the last part, we made a subset of the `penguins` data called `gentoo`, which was only the cases for which `species == \"Gentoo\"`. Do that again and refit the Bayesian model to those data. Can you then remake some of the figures in this file with the new version of the model?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(gentoo_b) %>% \n  pivot_longer(starts_with(\"b_\")) %>% \n  ggplot(aes(x = value)) + \n  geom_histogram(bins = 40) +\n  facet_wrap(~ name, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(gentoo_b, off_diag_args = list(size = 1/4, alpha = 1/4))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws_gentoo <- as_draws_df(gentoo_b)\ndraws_gentoo <- draws_gentoo %>% \n  mutate(beta0 = b_Intercept,\n         beta1 = b_body_mass_g)\n\ndraws_gentoo %>% \n  select(.draw, beta0, beta1) %>% \n  mutate(body_mass_g = mean(gentoo$body_mass_g)) %>% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %>% \n  ggplot(aes(x = y_hat)) +\n  geom_histogram(bins = 40) +\n  labs(title = \"Bayesians have posterior distributions\",\n       x = expression(hat(italic(y))*'|'*italic(x)==mean(gentoo$body_mass_g))) +\n  coord_cartesian(xlim = c(45, 50))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npoints_gentoo <- draws_gentoo %>% \n  rename(`beta[0]` = beta0,\n         `beta[1]` = beta1) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  group_by(parameter) %>% \n  summarise(mean = mean(value),\n            median = median(value),\n            mode = Mode(value)) %>% \n  pivot_longer(starts_with(\"m\"), names_to = \"statistic\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n```{.r .cell-code}\ndraws_gentoo %>% \n  rename(`beta[0]` = beta0,\n         `beta[1]` = beta1) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  ggplot(aes(x = value)) +\n  geom_density() +\n  geom_vline(data = points_gentoo,\n             aes(xintercept = value, color = statistic),\n             size = 3/4) +\n  scale_color_viridis_d(option = \"A\", end = .8) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, labeller = label_parsed, scales = \"free\", ncol = 1) +\n  theme(strip.text = element_text(size = 14))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\ndraws_gentoo %>% \n  slice_sample(n = 20) %>% \n  select(.draw, beta0, beta1) %>% \n  expand_grid(body_mass_g = range(gentoo$body_mass_g)) %>% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %>% \n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw, color = beta0)) +\n  geom_line() +\n  scale_color_viridis_c(expression(beta[0]~(the~intercept)), end = .9)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Bayes_Lab_1_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n\n\n## References\n\nGelman, A., Hill, J., & Vehtari, A. (2020). *Regression and other stories*. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\nKruschke, J. K. (2015). *Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan*. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\nMcElreath, R. (2020). *Statistical rethinking: A Bayesian course with examples in R and Stan* (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\nMcElreath, R. (2015). *Statistical rethinking: A Bayesian course with examples in R and Stan*. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n## Session information\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.3 (2025-02-28)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.5.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggdist_3.3.2         palmerpenguins_0.1.1 broom.mixed_0.2.9.6 \n [4] broom_1.0.7          brms_2.22.0          Rcpp_1.0.14         \n [7] ggside_0.3.1         lubridate_1.9.4      forcats_1.0.0       \n[10] stringr_1.5.1        dplyr_1.1.4          purrr_1.0.4         \n[13] readr_2.1.5          tidyr_1.3.1          tibble_3.2.1        \n[16] ggplot2_3.5.1        tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     viridisLite_0.4.2    farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] digest_0.6.37        estimability_1.5.1   timechange_0.3.0    \n[10] lifecycle_1.0.4      StanHeaders_2.32.10  processx_3.8.6      \n[13] magrittr_2.0.3       posterior_1.6.1      compiler_4.4.3      \n[16] rlang_1.1.5          tools_4.4.3          utf8_1.2.4          \n[19] yaml_2.3.10          knitr_1.50           labeling_0.4.3      \n[22] bridgesampling_1.1-2 htmlwidgets_1.6.4    curl_6.2.1          \n[25] pkgbuild_1.4.7       plyr_1.8.9           abind_1.4-8         \n[28] withr_3.0.2          grid_4.4.3           stats4_4.4.3        \n[31] colorspace_2.1-1     future_1.34.0        inline_0.3.21       \n[34] emmeans_1.11.0       globals_0.16.3       scales_1.3.0        \n[37] cli_3.6.4            mvtnorm_1.3-3        rmarkdown_2.29      \n[40] generics_0.1.3       RcppParallel_5.1.10  rstudioapi_0.17.1   \n[43] reshape2_1.4.4       tzdb_0.5.0           rstan_2.32.7        \n[46] splines_4.4.3        bayesplot_1.11.1     parallel_4.4.3      \n[49] matrixStats_1.5.0    vctrs_0.6.5          V8_6.0.2            \n[52] Matrix_1.7-2         jsonlite_1.9.1       callr_3.7.6         \n[55] hms_1.1.3            listenv_0.9.1        glue_1.8.0          \n[58] parallelly_1.42.0    ps_1.9.0             codetools_0.2-20    \n[61] distributional_0.5.0 stringi_1.8.4        gtable_0.3.6        \n[64] QuickJSR_1.7.0       munsell_0.5.1        furrr_0.3.1         \n[67] pillar_1.10.1        htmltools_0.5.8.1    Brobdingnag_1.2-9   \n[70] R6_2.6.1             evaluate_1.0.3       lattice_0.22-6      \n[73] backports_1.5.0      rstantools_2.4.0     coda_0.19-4.1       \n[76] gridExtra_2.3        nlme_3.1-167         checkmate_2.3.2     \n[79] mgcv_1.9-1           xfun_0.51            pkgconfig_2.0.3     \n```\n\n\n:::\n:::\n",
    "supporting": [
      "Bayes_Lab_1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}