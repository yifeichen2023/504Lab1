{
  "hash": "28dbefebc6daef5ecb72175d0c7ada9b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"PSY 504: Bayes Lab 3_2, HMC Diagnostics April 16th, 2025\"\nformat: html\neditor: visual\n---\n\n\n\nThis worksheet helps to give you a better idea about what to do with the trace plots.\n\n## Packages and data\n\nLoad the primary packages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(faux)\nlibrary(GGally)\nlibrary(brms)\nlibrary(ggmcmc)\nlibrary(bayesplot)\n```\n:::\n\n\n\nThis time we'll simulate data with the **faux** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many cases?\nn <- 100\n\n# population values\nmu    <- 0\nsigma <- 1\nrho   <- .5\n\n# simulate and save\nset.seed(1)\n\nd <- rnorm_multi(\n  n = n,\n  mu = c(mu, mu),\n  sd = c(sigma, sigma), \n  r = rho, \n  varnames = list(\"x\", \"y\")\n)\n\nglimpse(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 2\n$ x <dbl> -0.232341576, 0.137981847, -0.268214782, 1.302539315, 0.612654423, -…\n$ y <dbl> -0.85270825, 0.18009772, -1.17913643, 1.46056809, -0.04193022, 0.173…\n```\n\n\n:::\n:::\n\n\n\nWe might look at the data with a `ggpairs()` plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  ggpairs(diag = list(continuous = wrap(\"barDiag\", binwidth = 0.25)),\n          upper = list(continuous = wrap(\"cor\", stars = FALSE)))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nCheck the sample statistics.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# univariate\nd %>% \n  pivot_longer(everything()) %>% \n  group_by(name) %>% \n  summarise(m = mean(value),\n            s = sd(value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  name       m     s\n  <chr>  <dbl> <dbl>\n1 x     0.113  0.914\n2 y     0.0754 0.913\n```\n\n\n:::\n\n```{.r .cell-code}\n# bivariate\nd %>% \n  summarise(r = cor(y, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          r\n1 0.4502206\n```\n\n\n:::\n:::\n\n\n\n## Base model\n\nLet's fit a simple model\n\n$$\n\\begin{align}\ny_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 x_i \\\\\n\\beta_0 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1),\n\\end{align}\n$$\n\nAs we fit the model with `brm()`, take the opportunity to consider some of the default settings.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit13.b <- brm(\n  data = d,\n  family = gaussian,\n  y ~ 1 + x,\n  prior = prior(normal(0, 1), class = Intercept) +\n    prior(normal(0, 1), class = b) +\n    prior(exponential(1), class = sigma),\n  seed = 13,\n  \n  # default settings we've been ignoring up to this point\n  iter = 2000, warmup = 1000, chains = 4, cores = 1\n  # if you have a good computer, maybe try setting cores = 4\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\nIf you'd like to use multiple cores, but you're not sure how many you have, execute `parallel::detectCores()`.\n\n#### Question 1: How many cores do you have?\n\n10\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparallel::detectCores()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n:::\n\n\n\nCheck the model summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit13.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.08    -0.14     0.18 1.00     3741     3084\nx             0.45      0.09     0.27     0.62 1.00     4125     3260\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.06     0.72     0.95 1.00     3939     3163\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nLook at the parameter posteriors in a `pairs()` plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(fit13.b, \n      off_diag_args = list(size = 1/3, alpha = 1/3))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nThe `pairs()` plot is a wrapper around the `mcmc_pairs()` function from **bayesplot**. By default, half of the chains are depicted in the scatter plots below the diagonal, and the other half are displayed above the diagonal. The basic idea is you want the results form different chains to mirror one another. You can control this behavior with the `condition` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(fit13.b, \n      off_diag_args = list(size = 1/3, alpha = 1/3),\n      # here we put the first chain in above the diagonal,\n      # and we put the second through fourth chains below the diagonal\n      condition = pairs_condition(chains = list(1, 2:4)))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nThis particular arrangement is a little silly, but it should give you a sense of how to control the output. Also, by default the histograms on the diagonal use the draws from all the chains.\n\nIf you wanted, you could also make a similar kind of plot with `ggpairs()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit13.b) %>% \n  select(b_Intercept:sigma) %>% \n  ggpairs(diag = list(continuous = wrap(\"barDiag\", bins = 25)),\n          upper = list(continuous = wrap(\"cor\", stars = FALSE)),\n          lower = list(continuous = wrap(\"points\", size = 1/4, alpha = 1/3)))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nNow take a look at the `plot()` output.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit13.b, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nThese trace plots look like a dream. They have the appearance of fuzzy caterpillars, which is why they're even sometimes called *caterpillar plots*.\n\nLet's work directly with the chains via `as_draws_df()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit13.b) %>% \n  # notice the 3 meta-data columns at the end\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,000\nColumns: 9\n$ b_Intercept <dbl> 0.023961191, 0.008764307, -0.066725637, -0.085990390, 0.01…\n$ b_x         <dbl> 0.5574909, 0.5822172, 0.4748096, 0.4267265, 0.4081808, 0.4…\n$ sigma       <dbl> 0.8455813, 0.8642759, 0.7653377, 0.9147409, 0.8412759, 0.7…\n$ Intercept   <dbl> 0.08707099, 0.07467320, -0.01297564, -0.03768355, 0.063654…\n$ lprior      <dbl> -2.842647, -2.874429, -2.716021, -2.844376, -2.764485, -2.…\n$ lp__        <dbl> -124.8296, -125.3474, -125.0186, -126.0295, -124.1495, -13…\n$ .chain      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ .iteration  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ .draw       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n```\n\n\n:::\n:::\n\n\n\nWe can use those meta-data columns to make our own trace plots with **ggplot** functions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit13.b) %>% \n  pivot_longer(b_Intercept:sigma) %>% \n  mutate(.chain = factor(.chain),\n         # not needed, but makes for Greek formatted strip labels\n         greek = case_when(\n    name == \"b_Intercept\" ~ \"beta[0]\",\n    name == \"b_x\"         ~ \"beta[1]\",\n    name == \"sigma\"       ~ \"sigma\"\n  )) %>% \n  \n  ggplot(aes(x = .iteration, y = value, color = .chain)) +\n  geom_line(linewidth = 1/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  ggtitle(\"Hand-made trace plots!\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nWe might restrict to the first few post-warmup iterations to help give us a better sense of what's happening.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit13.b) %>% \n  filter(.iteration < 21) %>% \n  pivot_longer(b_Intercept:sigma) %>% \n  mutate(.chain = factor(.chain),\n         # not needed, but makes for nice formatting\n         greek = case_when(\n    name == \"b_Intercept\" ~ \"beta[0]\",\n    name == \"b_x\"         ~ \"beta[1]\",\n    name == \"sigma\"       ~ \"sigma\"\n  )) %>% \n  \n  ggplot(aes(x = .iteration, y = value, color = .chain)) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  ggtitle(\"Hand-made trace plots (zoomed in)\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nNote that these are all post-warmup draws. The **brms** package doesn't make it easy to visualize the warmup draws. But we can do so with a little help from the **ggmcmc** package's `ggs()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first execute without summarise()\nggs(fit13.b) %>% \n  summarise(min = min(Iteration),\n            max = max(Iteration))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n    min   max\n  <int> <int>\n1     1  2000\n```\n\n\n:::\n:::\n\n\n\nNote how how the values in the `Iteration` column range from 1 to 2,000. By **brms** default, the first 1,000 of those iterations are the warmup's. Here is how we can use the `ggs()` output to make trace plots that include the warmup draws.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggs(fit13.b) %>% \n  filter(Parameter != \"lprior\") %>% \n  mutate(Chain = factor(Chain),\n         greek = case_when(\n    Parameter == \"b_Intercept\" ~ \"beta[0]\",\n    Parameter == \"b_x\"         ~ \"beta[1]\",\n    Parameter == \"sigma\"       ~ \"sigma\"\n  )) %>% \n  \n  ggplot(aes(x = Iteration, y = value, color = Chain)) +\n  # this marks off the warmups\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = \"black\", alpha = 1/6, linewidth = 0) +\n  geom_line(linewidth = 1/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  labs(title = \"More hand-made trace plots\",\n       subtitle = \"warmup/post-warmup by background\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nLet's take a closer look at the first few warmup iterations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggs(fit13.b) %>% \n  filter(Parameter != \"lprior\") %>% \n  mutate(Chain = factor(Chain),\n         greek = case_when(\n    Parameter == \"b_Intercept\" ~ \"beta[0]\",\n    Parameter == \"b_x\"         ~ \"beta[1]\",\n    Parameter == \"sigma\"       ~ \"sigma\"\n  )) %>% \n  \n  ggplot(aes(x = Iteration, y = value, color = Chain)) +\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = \"black\", alpha = 1/6, linewidth = 0) +\n  geom_line(linewidth = 2/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  coord_cartesian(xlim = c(0, 50)) +\n  labs(title = \"More hand-made trace plots (zoomed in)\",\n       subtitle = \"warmup only\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n#### Question 2: Can you use the results here to describe the need for discarding warmup draws?\n\nIt is needed because at the very beginning, the algorithm is still very unstable and exploring a wide variety of options. It is not converging until later on (when the lines overlap with each other a lot more).\n\nAnother issue is *autocorrelation*, the degree to which a given HMC draw is correlated with the previous draw(s). We can make a plot of the autocorrelations with the `mcmc_acf()` function from the **bayesplot** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit13.b %>% \n  mcmc_acf(pars = vars(b_Intercept, b_x, sigma),\n           lags = 10)  # lags = 20 is the default\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\nThis is what we like to see: Nice L-shaped autocorrelation plots. Low autocorrelations like this are one of the major achievements of Stan's implementation of HMC. It's not uncommon for MCMC via the older Gibbs sampler method to routinely show much higher autocorrelations. You can get a sense of this by comparing the various models in Kruschke's (2015) textbook, which often uses the Gibbs sampler, versus their `brms()` analogues in my (2023) ebook translation.\n\n::: callout-note\nMixing describes how efficiently MCMC chains explore the posterior distribution. Good mixing means samples move freely across the parameter space. And high autocorrelation =\\> poor mixing.\n:::\n\n#### Question 3: Why are L-shaped autocorrelation plots are desirable? What would an undesirable autocorrelation plot look like?\n\nHaving the autocorrelation going down/near zero is good because it means that each consecutive draw is independent of each other, meaning that for each draw, we are able to get almost entirely new info -\\> making it more effective.\n\nThose low autocorrelations also have a lot to do with our effective sample size (ESS) estimates. Take another look at the `summary()` output.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit13.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.08    -0.14     0.18 1.00     3741     3084\nx             0.45      0.09     0.27     0.62 1.00     4125     3260\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.06     0.72     0.95 1.00     3939     3163\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nThere used to be a single ESS column. Starting with version 2.10.0, **brms** returns two columns: `Bulk_ESS` and `Tail_ESS`. These originate from Vehtari et al (2019). From the paper, we read:\n\n> When reporting quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3, we show that convergence of Markov chains is not uniform across the parameter space, that is, convergence might be different in the bulk of the distribution (e.g., for the mean or median) than in the tails (e.g., for extreme quantiles). We propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as bulk-ESS), which mainly assesses how well the centre of the distribution is resolved. Instead, these \"tail-ESS\" measures allow the user to estimate the MCSE for interval estimates. (pp. 672-673)\n\nWe generally like the values in both the `Bulk_ESS` and `Tail_ESS` columns to be as close to the total number of post-warmup draws as possible, which would be 4,000 for a default `brm()` model. Sometimes, as in the case of the `Bulk_ESS` value for our $\\beta_1$ parameter, the HMC chains are so efficient that we can get larger numbers than the actual number of post-warmup draws. This is related to when we have negative autocorrelations (see above).\n\nHow much is enough, and how low is too low? Yeah, indeed... Higher is generally better, with diminishing returns rolling in somewhere between 1,000 and 10,000. **brms** will give you a warning message when the ESS estimates get below a couple hundred.\n\nNow look back at the `Rhat` column in the `summary()` output. This is the potential scale reduction factor $\\hat R$. It has its origins in Gelman & Rubin (1992), but the current version used in **brms** is from Vehtari et al (2019), as cited above. In short, it is something of a ratio of the between-chain variation versus the within-chain variation. This ratio is usually a little above 1, and we want it to be as close to 1 as possible. The Stan team (e.g., <https://mc-stan.org/rstan/reference/Rhat.html)> recommends against values greater than 1.05. In our case, we're good to go.\n\n## What bad chains look like..\n\nNow let's break the model. This time, we'll subset the `d` data to just the first 2 rows, we'll make the priors very wide on the scale of the data, and we'll dramatically reduce the `warmup` period.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit14.b <- brm(\n  data = d %>% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  # don't use priors like this for real data analyses\n  prior = prior(normal(0, 100000), class = Intercept) +\n    prior(normal(0, 100000), class = b) +\n    prior(uniform(0, 100000), class = sigma),\n  seed = 14,\n  iter = 1100, warmup = 100, chains = 4, cores = 4\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: It appears as if you have specified an upper bounded prior on a parameter that has no natural upper bound.\nIf this is really what you want, please specify argument 'ub' of 'set_prior' appropriately.\nWarning occurred for prior \n<lower=0> sigma ~ uniform(0, 1e+05)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 801 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 1855 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Examine the pairs() plot to diagnose sampling problems\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The largest R-hat is 1.88, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n```\n\n\n:::\n:::\n\n\n\nCheck the parameter summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit14.b)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Parts of the model have not converged (some Rhats are > 1.05). Be\ncareful when analysing the results! We recommend running more iterations and/or\nsetting stronger priors.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 801 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d %>% slice(1:2) (Number of observations: 2) \n  Draws: 4 chains, each with iter = 1100; warmup = 100; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   478.83   2036.34 -1979.74  6652.81 1.46        8       13\nx         -1228.64   3124.10 -6776.94  5668.31 1.88        6       15\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma  6231.76  11923.16   165.35 42287.98 1.13       22      336\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nNever ignore Warning messages like that.\n\nThose `Rhat`, `Bulk_ESS`, and `Tail_ESS` look really bad. Also notice how large the posterior means (`Estimate`) and standard deviations (`Est.Error`) are. Seems off, eh?\n\nLet's investigate further with a `pairs()` plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit14.b, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\nThis is a full-scale disaster. DO NOT trust model results from chains that look like this.\n\nIn this case, just giving the model a longer `warmup` period helped a lot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit15.b <- brm(\n  data = d %>% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  # don't use priors like this in real life\n  prior = prior(normal(0, 100000), class = Intercept) +\n    prior(normal(0, 100000), class = b) +\n    prior(uniform(0, 100000), class = sigma),\n  seed = 14,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: It appears as if you have specified an upper bounded prior on a parameter that has no natural upper bound.\nIf this is really what you want, please specify argument 'ub' of 'set_prior' appropriately.\nWarning occurred for prior \n<lower=0> sigma ~ uniform(0, 1e+05)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 1525 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Examine the pairs() plot to diagnose sampling problems\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit15.b, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\nWe still have a lot of Warning messages, but things have improved.\n\nWe can do an even better with default weakly-regularizing priors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit16.b <- brm(\n  data = d %>% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  prior = prior(normal(0, 1), class = Intercept) +\n    prior(normal(0, 1), class = b) +\n    prior(exponential(1), class = sigma),\n  seed = 14,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 22 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Examine the pairs() plot to diagnose sampling problems\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit16.b, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_3_2_HMC-Diagnostics_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\nNow look at the parameter summaries.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit16.b)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There were 22 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d %>% slice(1:2) (Number of observations: 2) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.23      0.49    -1.21     0.83 1.00     1810     1643\nx             0.47      0.99    -1.46     2.38 1.00     1955     1932\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.86      0.59     0.19     2.45 1.00     1065      982\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nThose Warning messages still remain, but they're less dire than before. Also, most of the other diagnostics look better. I still wouldn't trust this model. It is only based on 2 data points, after all. But look how far we got by paying attention to the diagnostics and picking better priors.\n\n## References\n\nGelman, A. and Rubin, D. (1992). Inference from iterative simulation using multiple sequences. *Statistical Science, 7*(4):457–472. <https://dx.doi.org/10.1214/ss/1177011136>\n\nKruschke, J. K. (2015). *Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan*. Academic Press. <https://sites.google.com/site/doingbayesiandataanalysis/>\n\nKurz, A. S. (2023). *Doing Bayesian data analysis in brms and the tidyverse* (Version 1.1.0). <https://bookdown.org/content/3686/>\n\nMcElreath, R. (2020). *Statistical rethinking: A Bayesian course with examples in R and Stan* (Second Edition). CRC Press. <https://xcelab.net/rm/statistical-rethinking/>\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved $\\widehat R$ for assessing convergence of MCMC (with discussion). *Bayesian Analysis, 16*(2), 667-718. <https://doi.org/10.1214/20-BA1221>\n\n## Session information\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.3 (2025-02-28)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.5.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] bayesplot_1.11.1 ggmcmc_1.5.1.1   brms_2.22.0      Rcpp_1.0.14     \n [5] GGally_2.2.1     faux_1.2.2       lubridate_1.9.4  forcats_1.0.0   \n [9] stringr_1.5.1    dplyr_1.1.4      purrr_1.0.4      readr_2.1.5     \n[13] tidyr_1.3.1      tibble_3.2.1     ggplot2_3.5.2    tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     viridisLite_0.4.2    farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] digest_0.6.37        estimability_1.5.1   timechange_0.3.0    \n[10] lifecycle_1.0.4      StanHeaders_2.32.10  processx_3.8.6      \n[13] magrittr_2.0.3       posterior_1.6.1      compiler_4.4.3      \n[16] rlang_1.1.5          tools_4.4.3          utf8_1.2.4          \n[19] yaml_2.3.10          knitr_1.50           labeling_0.4.3      \n[22] bridgesampling_1.1-2 htmlwidgets_1.6.4    pkgbuild_1.4.7      \n[25] curl_6.2.1           plyr_1.8.9           RColorBrewer_1.1-3  \n[28] abind_1.4-8          withr_3.0.2          grid_4.4.3          \n[31] stats4_4.4.3         colorspace_2.1-1     inline_0.3.21       \n[34] emmeans_1.11.0       scales_1.3.0         cli_3.6.4           \n[37] mvtnorm_1.3-3        rmarkdown_2.29       generics_0.1.3      \n[40] RcppParallel_5.1.10  rstudioapi_0.17.1    reshape2_1.4.4      \n[43] tzdb_0.5.0           rstan_2.32.7         parallel_4.4.3      \n[46] matrixStats_1.5.0    vctrs_0.6.5          V8_6.0.2            \n[49] Matrix_1.7-2         jsonlite_1.9.1       callr_3.7.6         \n[52] hms_1.1.3            glue_1.8.0           ggstats_0.9.0       \n[55] codetools_0.2-20     ps_1.9.0             distributional_0.5.0\n[58] stringi_1.8.4        gtable_0.3.6         QuickJSR_1.7.0      \n[61] munsell_0.5.1        pillar_1.10.1        htmltools_0.5.8.1   \n[64] Brobdingnag_1.2-9    R6_2.6.1             evaluate_1.0.3      \n[67] lattice_0.22-6       backports_1.5.0      rstantools_2.4.0    \n[70] coda_0.19-4.1        gridExtra_2.3        nlme_3.1-167        \n[73] checkmate_2.3.2      xfun_0.51            pkgconfig_2.0.3     \n```\n\n\n:::\n:::\n",
    "supporting": [
      "Bayes_Lab_3_2_HMC-Diagnostics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}