{
  "hash": "864d1744931ab3ddcdbee4d7f0a75b9a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayes_Lab_2\"\nformat: html\neditor: visual\n---\n\n\n\nFor Lab 1, you had explored the data and looked at models built via lm() and via brms(using default priors). You had also drawn posterior samples after fitting the model.\n\nFor Lab 2, we continue with the Palmer Penguins. And we will look more at distributions and priors.\n\nAgain, there will be conceptual questions to answer as you work through this example, and exercises.\n\n# Part 3: Distributions all the way down\n\nGiven it's a continuation of Lab 1, let's begin by loading relevant packages, cleaning/pre-processing the data, and fitting lm() and the default brm models\n\n## Setup: Packages and data\n\nWe load the primary packages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\n```\n:::\n\n\n\nWe want the same data set up as in the last lab.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the penguins data\ndata(penguins, package = \"palmerpenguins\")\n\n# subset the data\nchinstrap <- penguins %>% \n  filter(species == \"Chinstrap\")\n\nglimpse(chinstrap)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 68\nColumns: 8\n$ species           <fct> Chinstrap, Chinstrap, Chinstrap, Chinstrap, Chinstra…\n$ island            <fct> Dream, Dream, Dream, Dream, Dream, Dream, Dream, Dre…\n$ bill_length_mm    <dbl> 46.5, 50.0, 51.3, 45.4, 52.7, 45.2, 46.1, 51.3, 46.0…\n$ bill_depth_mm     <dbl> 17.9, 19.5, 19.2, 18.7, 19.8, 17.8, 18.2, 18.2, 18.9…\n$ flipper_length_mm <int> 192, 196, 193, 188, 197, 198, 178, 197, 195, 198, 19…\n$ body_mass_g       <int> 3500, 3900, 3650, 3525, 3725, 3950, 3250, 3750, 4150…\n$ sex               <fct> female, male, male, female, male, female, female, ma…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n\n\n:::\n:::\n\n\n\n## Models\n\nOnce again, we'll fit the model\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) ,\n\\end{align}\n$$\n\nwith both `lm()` and `brm()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS\nfit1.ols <- lm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n# Bayes\nfit1.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Estimate    Est.Error          Q2.5         Q97.5\nb_Intercept    3.217753e+01 3.5185513514  2.536101e+01  3.913705e+01\nb_body_mass_g  4.465013e-03 0.0009387541  2.626183e-03  6.282621e-03\nsigma          2.932174e+00 0.2598294337  2.480331e+00  3.481208e+00\nIntercept      4.884582e+01 0.3711428007  4.814181e+01  4.958049e+01\nlprior        -4.300564e+00 0.0706590358 -4.459214e+00 -4.183931e+00\nlp__          -1.723671e+02 1.3242335672 -1.757457e+02 -1.708953e+02\n```\n\n\n:::\n:::\n\n\n\n## Bayesians have many kinds of distributions\n\nIn Bayesian statistics, we have at least 6 distributions to keep track of. Those are:\n\n-   the likelihood distributions\n-   the prior parameter distribution (aka priors)\n-   the prior predictive distributions\n-   the posterior parameter distributions (aka posteriors)\n-   the posterior-predictive distribution\n\nIn many respect, it's distributions 'all the way down,' with Bayesians. This can be indeed be difficult to keep track of at first. But since this is true for any class of Bayesian models (not just regression), you'll hopefully get used to it.\\\n\n### QUESTION 1: How would you represent these 6 distributions mathematically, using $P_0$'$P$, $D$, $|$, and $\\theta$ ?\n\n::: callout-tip\nHint 1: Many of these terms were in the Bayes Rule.\n:::\n\n### Answer: \n\n-   likelihood distribution \\~ P (D\\|θ)\n\n-   priors \\~ P_0(θ)\n\n-   prior predictive distribution \\~ P_0(D)\n\n-   posterior parameter distributions (posterior) \\~ P(θ\\|D)\n\n-   posterior predictive distribution \\~ P(D_new\\|D_old)\n\nWe also have some other distributions that follow from these. For example, - the distributions of the model expectations (i.e., the predicted means)\n\n### Likelihood distributions.\n\nWe are approaching Bayesian statistics from a likelihood-based perspective. That is, we situate regression models within the greater context of a likelihood function. (There are ways to do non-parametric Bayesian statistics, which don't focus on likelihoods. We won't get into that right now.)\n\nSo far, we have been using the conventional Gaussian likelihood. If we have some variable $y$, we can express it as normally distributed by\n\n$$\n\\operatorname{Normal}(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( \\frac{1}{2} \\left( \\frac{y - \\mu}{\\sigma}\\right)^2\\right),\n$$\n\nwhere $\\mu$ is the mean and $\\sigma$ is the standard deviation. With this likelihood,\n\n-   $\\mu \\in \\mathbb R$\n    -   the mean can be any real number, ranging from $-\\infty$ to $\\infty$\n-   $\\sigma \\in \\mathbb R_{> 0}$\n    -   the standard deviation can take on any real number greater than zero.\n\nIt's also the assumption\n\n-   $y \\in \\mathbb R$\n    -   the focal variable $y$ can be any real number, ranging from $-\\infty$ to $\\infty$.\n\nOne of the ways we wrote our model formula back in the first file was\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i,\n\\end{align}\n$$\n\nand further in the discussion, we updated that equation with the posterior means for our three parameters to\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, 2.92) \\\\\n\\mu_i & = 32.2 + 0.004 \\text{body_mass_g}_i.\n\\end{align}\n$$\n\nBefore we get into this, though, let's back up and consider an intercept-only model of the form\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 ,\n\\end{align}\n$$\n\nwhere there is no predictor variable. Here's how to fit the model with `brm()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bayes\nfit0.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n\nLet's look at the model summary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit0.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    48.85      0.41    48.06    49.64 1.00     3823     2652\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     3.37      0.29     2.87     4.00 1.00     3440     2846\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\nThe intercept parameter $\\beta_0$ is a stand-in for $\\mu$. The $\\sigma$ parameter is just $\\sigma$. Here they are in a plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- as_draws_df(fit0.b) \n\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nHere are the posterior means for those two parameters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- mean(draws$b_Intercept)\nsigma <- mean(draws$sigma)\n\nmu; sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 48.85113\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.372572\n```\n\n\n:::\n:::\n\n\n\nWe can use `dnorm()` to compute the shape of $\\operatorname{Normal}(48.8, 3.4)$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(y = seq(from = 30, to = 70, by = 0.1)) %>% \n  mutate(density = dnorm(x = y, mean = mu, sd = sigma)) %>% \n  \n  ggplot(aes(x = y, y = density)) +\n  geom_line() +\n  xlab(\"bill_length_mm\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nWe can compare this to the sample distribution of the `bill_length_mm` data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap %>% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 2.5) +\n  geom_line(data = tibble(bill_length_mm = seq(from = 30, to = 70, by = 0.1)),\n            aes(y = dnorm(x = bill_length_mm, mean = mu, sd = sigma)),\n            color = \"red\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nIt's not a great fit, but not horrible either.\n\nNow let's see what this means for our univariable model `fit1.b`. First, let's learn about the `posterior_summary()` function, which we'll use to save a few posterior means.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Estimate    Est.Error          Q2.5         Q97.5\nb_Intercept    3.217753e+01 3.5185513514  2.536101e+01  3.913705e+01\nb_body_mass_g  4.465013e-03 0.0009387541  2.626183e-03  6.282621e-03\nsigma          2.932174e+00 0.2598294337  2.480331e+00  3.481208e+00\nIntercept      4.884582e+01 0.3711428007  4.814181e+01  4.958049e+01\nlprior        -4.300564e+00 0.0706590358 -4.459214e+00 -4.183931e+00\nlp__          -1.723671e+02 1.3242335672 -1.757457e+02 -1.708953e+02\n```\n\n\n:::\n\n```{.r .cell-code}\nb0    <- posterior_summary(fit1.b)[1, 1]\nb1    <- posterior_summary(fit1.b)[2, 1]\nsigma <- posterior_summary(fit1.b)[3, 1]\n```\n:::\n\n\n\nNow we plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(body_mass_g    = seq(from = 2500, to = 5000, length.out = 200),\n         bill_length_mm = seq(from = 35, to = 60, length.out = 200))  %>% \n  mutate(density = dnorm(x = bill_length_mm, \n                         mean = b0 + b1 * body_mass_g,\n                         sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_raster(aes(fill = density),\n              interpolate = TRUE) +\n  geom_point(data = chinstrap,\n             shape = 21, color = \"white\", fill = \"black\", stroke = 1/4) +\n  scale_fill_viridis_c(option = \"A\", begin = .15, limits = c(0, NA)) +\n  coord_cartesian(xlim = range(chinstrap$body_mass_g),\n                  ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nOur univariable model `fit1.b` can be viewed as something like a 3-dimensional Gaussian hill.\n\n### Prior distributions & Prior predictive distributions.\n\nLet's hold off on this for a bit.\n\n### Parameter distributions.\n\nUp above, we plotted the posterior distributions for our intercept-only `fit0.b` model. Here they are again.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .99, normalize = \"panels\",\n               # customize some of the aesthetics\n               fill = \"lightskyblue1\", color = \"royalblue\", \n               point_color = \"darkorchid4\", point_size = 4, shape = 15) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit0.b\",\n       subtitle = \"This time we used 99% intervals, and got silly with the colors.\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nWe might practice making a similar plot for our univariable model `fit1.b`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit1.b) %>% \n  rename(`beta[0]` = b_Intercept,\n         `beta[1]` = b_body_mass_g) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit1.b\",\n       subtitle = \"Using good old 95% intervals, but switching to histograms\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nSome authors, like John Kruschke, have a strong preference for plotting their posteriors with histograms, rather than density plots.\n\n## Distributions of the model expectations.\n\nTake another look at the `conditional_effects()` plot from earlier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(fit1.b) %>% \n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nThe blue line is the posterior mean, for the $\\mu_i$, the model-based mean for `bill_length_mm`, given the value for the predictor `body_mass_g`. The semitransparent gray ribbon marks the percentile-based interval for the conditional mean.\n\nWe can make a similar plot with the `fitted()` function. First we'll need a predictor grid, we'll call `nd`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnd <- tibble(body_mass_g = seq(\n  from = min(chinstrap$body_mass_g),\n  to = max(chinstrap$body_mass_g),\n  length.out = 100))\n\nglimpse(nd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 1\n$ body_mass_g <dbl> 2700.000, 2721.212, 2742.424, 2763.636, 2784.848, 2806.061…\n```\n\n\n:::\n:::\n\n\n\nNow pump `nd` into the `fitted()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.23307 1.0332340 42.26531 46.30783\n[2,] 44.32778 1.0146750 42.39668 46.36310\n[3,] 44.42249 0.9961682 42.52449 46.41837\n[4,] 44.51720 0.9777167 42.65175 46.47754\n[5,] 44.61192 0.9593237 42.78476 46.53316\n[6,] 44.70663 0.9409925 42.90636 46.59023\n```\n\n\n:::\n:::\n\n\n\nNow plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nLook what happens if we augment the `probs` argument in `fitted()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       probs = c(.025, .975, .25, .75)) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  # 95% range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 50% range\n  geom_ribbon(aes(ymin = Q25, ymax = Q75),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\nNow look what happens if we set `summary = FALSE`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = FALSE) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:4000, 1:100] 44.2 44.3 44.6 43.7 42.6 ...\n```\n\n\n:::\n:::\n\n\n\nWe get full 4,000 draw posterior distributions for each of the 100 levels of the predictor `body_mass_g`. Now look at what happens if we wrangle that output a little, and plot with aid from `stat_lineribbon()` from the **ggdist** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  stat_lineribbon() +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\nLook what happens when we request more intervals in the `.width` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  # make more ribbons\n  stat_lineribbon(.width = c(.1, .2, .3, .4, .5, .6, .7, .8, .9),\n                  # remove the line\n                  linewidth = 0) +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\nThe conditional mean, $\\mu_i$, has its own distribution. We can take this visualization approach even further to make a color gradient.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value, fill = after_stat(.width))) +\n  # make more ribbons\n  stat_lineribbon(.width = ppoints(50)) +\n  scale_fill_distiller(limits = 0:1) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\nFor technical details on this visualization approach, go here: <https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-gradients>.\n\nThe **ggdist** package even has an experimental visualization approach that's based on density gradients, rather than interval-width gradients. Since this is experimental, I'm not going to go into the details. But if you're curious and adventurous, you can learn more here: <https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-density-gradients>.\n\n### Posterior-predictive distributions.\n\nThe last section showed the posterior distributions for the model expectations (i.e., the conditional means). In the context of the Gaussian distribution, that's $\\mu$, or $\\mu_i$ in the case of the univariable model `fit1.b`. But the whole Gaussian distribution includes $\\mu$ and $\\sigma$.\n\nThis is where the `predict()` function comes in. First, we compare the `fitted()` output to `predict()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.23307 1.0332340 42.26531 46.30783\n[2,] 44.32778 1.0146750 42.39668 46.36310\n[3,] 44.42249 0.9961682 42.52449 46.41837\n[4,] 44.51720 0.9777167 42.65175 46.47754\n[5,] 44.61192 0.9593237 42.78476 46.53316\n[6,] 44.70663 0.9409925 42.90636 46.59023\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.24835  3.185415 38.04652 50.53301\n[2,] 44.25923  3.143345 38.04964 50.43316\n[3,] 44.44828  3.027479 38.38795 50.39715\n[4,] 44.51656  3.138462 38.48194 50.77914\n[5,] 44.63323  3.107523 38.28555 50.65644\n[6,] 44.61252  3.086634 38.60401 50.77928\n```\n\n\n:::\n:::\n\n\n\nThe posterior means (`Estimate`) are about the same, but the SD's (`Est.Error`) are much larger in the `predict()` output, and the widths of the 95% intervals are too. Let's make a plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\nThe gray band is the 95% interval for the entire posterior predictive distribution, not just the mean. In a good model, about 95% of the data points should be within those bands.\n\nDiscuss how the jagged lines have to do with the uncertainty in $\\sigma$.\n\nIf we wanted to, we could integrate the `fitted()`-based conditional posterior mean, with the `predict()`-based posterior-predictive distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save the fitted() results\nf <- fitted(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) \n\npredict(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% posterior-predictive range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 95% conditional mean range\n  geom_ribbon(data = f,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # posterior mean of the conditional mean\n  geom_line(data = f,\n            aes(y = Estimate)) +\n  # original data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\nIt's the posterior predictive distribution that we use to predict new data points. For example, here's what happens if we use `predict()` without the `newdata` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b) %>% \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 47.89312  2.990002 42.05939 53.86073\n[2,] 49.61706  2.956716 43.61991 55.49438\n[3,] 48.44201  2.999505 42.50396 54.18166\n[4,] 47.93548  2.986245 42.35192 54.05517\n[5,] 48.80950  2.924799 42.90281 54.38083\n[6,] 49.83768  2.968353 44.06186 55.71937\n```\n\n\n:::\n:::\n\n\n\nWe get posterior predictive summaries for each of the original data points. Here's what happens if we set `summary = FALSE`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, summary = FALSE) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:4000, 1:68] 50.7 40 51.6 48.3 46.1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n\n\nThis time, we got 4,000 posterior draws for each. We can reduce that output with the `ndraws` argument.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, summary = FALSE, ndraws = 6) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:6, 1:68] 47.4 43.8 56.4 46.2 44.5 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n\n\nNow wrangle and plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\npredict(fit1.b, summary = FALSE, ndraws = 6) %>% \n  data.frame() %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(row = str_remove(name, \"X\") %>% as.double()) %>% \n  left_join(chinstrap %>% \n              mutate(row = 1:n()),\n            by = join_by(row)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = value)) + \n  geom_point() +\n  ylab(\"bill_length_mm\") +\n  facet_wrap(~ draw, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\nWith `predict()`, we can use the entire posterior-predictive distribution to simulate new data based on the values of our predictor variable(s). To give you a better sense of what's happening under the hood, here's an `as_draws_df()` based alternative.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\n# walk this code through\nas_draws_df(fit1.b) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 6) %>% \n  expand_grid(chinstrap %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ .draw, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\nNow take a look at what happens when we plot the densities of several simulated draws.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\nas_draws_df(fit1.b) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 50) %>%  # increase the number of random draws\n  expand_grid(chinstrap %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = bill_length_mm, group = .draw)) + \n  geom_density(size = 1/4, color = alpha(\"black\", 1/2)) +\n  coord_cartesian(xlim = range(chinstrap$bill_length_mm) + c(-2, 2))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n\nThe similarities and differences among the individual density lines give you a sense of the (un)certainty of the posterior-predictive distribution.\n\n**This may be a good time for you to work on Exercise 1 (see end of the document)**\n\n#Part 4: Beginning to look at priors\n\n## Bayes' rule\n\nBayes' theorem will allow us to determine the plausibility of various values of our parameter(s) of interest, $\\theta$, given the data $d$, which we can express formally as $\\Pr(\\theta \\mid d)$. Bayes' rule takes on the form\n\n$$\n\\Pr(\\theta \\mid d) = \\frac{\\Pr(d \\mid \\theta) \\Pr(\\theta)}{\\Pr(d)}.\n$$\n\nwhere\n\n-   $\\Pr(d \\mid \\theta)$ is the *likelihood*,\n-   $\\Pr(\\theta)$ is the *prior*,\n-   $\\Pr(d)$ is the *average probability of the data*, and\n-   $\\Pr(\\theta \\mid d)$ is the *posterior*.\n\nWe can express this in words as\n\n$$\n\\text{Posterior} = \\frac{\\text{Probability of the data} \\times \\text{Prior}}{\\text{Average probability of the data}}.\n$$\n\nThe denominator $\\Pr(d)$ is a normalizing constant, and dividing by this constant is what converts the posterior $\\Pr(\\theta \\mid d)$ into a probability metric.\n\n## Default priors\n\nTo set your priors with **brms**, the `brm()` function has a `prior` argument. If you don't explicitly use the `prior` argument, `brm()` will use default priors. This is what happened with our `fit1.b` model from above. We used default priors. If you'd like to see what those priors are, execute `fit1.b$prior`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# maybe show str(fit1.b)\nfit1.b$prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\n\nThus, a fuller expression of our model is\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i \\\\\n\\beta_0 & \\sim \\operatorname{Student-t}(3, 49.5, 3.6) \\\\\n\\beta_1 & \\sim \\operatorname{Uniform}(-\\infty, \\infty) \\\\\n\\sigma & \\sim \\operatorname{Student-t}^+(3, 0, 3.6).\n\\end{align}\n$$\n\nIf we had wanted to see the `brm()` defaults before fitting the model, we could have used the `get_prior()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\n\nIf you recall, the normal distribution is a member of the Student-t family, where the $\\nu$ (aka degrees of freedom or normality parameter) is set to $\\infty$. To give you a sense, here are the densities of three members of the Student-t family, with varying $\\nu$ values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(theta = seq(from = -4.5, to = 4.5, length.out = 200),\n         nu = c(3, 10, Inf)) %>% \n  mutate(density = dt(x = theta, df = nu)) %>% \n  \n  ggplot(aes(x = theta, y = density, color = factor(nu))) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(expression(nu), option = \"A\", end = .7) +\n  labs(title = \"3 members of the Student-t family\",\n       x = expression(theta)) +\n  coord_cartesian(xlim = c(-4, 4))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\nThus, Student-t distributions have thicker tails when they have smaller $\\nu$ parameters. In the case where $\\nu = 3$, the tails are pretty thick, which means they are more tolerant of more extreme values. And thus priors with small-$\\nu$ parameters will be weaker (i.e., more permissive) than their Gaussian counterparts.\n\nWe can visualize functions from **ggdist** to visualize the default `brm()` priors. We'll start with the `student_t(3, 49.5, 3.6)` $\\beta_0$ prior, and also take the opportunity to compare that with a slightly stronger `normal(49.5, 3.6)` alternative.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(prior(student_t(3, 49.5, 3.6)),\n  prior(normal(49.5, 3.6))) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye() +\n  labs(x = expression(italic(p)(beta[0])),\n       y = NULL) +\n  coord_cartesian(xlim = c(25, 75))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\nSee how that $n = 3$ parameter in the default prior let do much thicker tails than it's Gaussian counterpart. We can make the same kind of plot for our default $\\sigma$ prior and its half-Gaussian counterpart.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(prior(student_t(3, 0, 3.6), lb = 0),  # note our use of the lb = 0 argument\n  prior(normal(0, 3.6), lb = 0)) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.90, .99)) +\n  labs(x = expression(italic(p)(sigma)),\n       y = NULL) +\n  coord_cartesian(xlim = c(0, 30))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\nHere's how we could have explicitly set our priors by hand.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g,\n  prior = prior(student_t(3, 49.5, 3.6), class = Intercept) +\n    prior(student_t(3, 0, 3.6), class = sigma, lb = 0)\n)\n```\n:::\n\n\n\nCompare the results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.18      3.52    25.36    39.14 1.00     4984     3109\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4956     2935\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.93      0.26     2.48     3.48 1.00     1921     1869\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit2.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.13      3.57    24.99    38.99 1.00     4691     2986\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4740     2970\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.94      0.27     2.47     3.52 1.00     2019     1693\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n## QUESTION 2 Are the priors the same? What do you think is going on?\n\n### Answer: ....\n\nI guess technically they are the same since the parameter values are the same except that for fit2 we manually define the prior whereas fit1 just use it as default. The slight difference is likely due to sampling variability.\n\nIf you want to learn more about the default prior settings for **brms**, read through the `set_prior` section of the **brms** reference manual (https://CRAN.R-project.org/package=brms/brms.pdf).\n\n# EXERCISE 1\n\nIn the previous lab, we made a subset of the `penguins` data called `gentoo`, which was only the cases for which `species == \"Gentoo\"`. Do that again and refit the Bayesian model to those data. Remake some of the figures (From Part 3) in this file with the new version of the model?\n\n### Answer/ Your solution below: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngentoo <- penguins %>%\n  filter(species == 'Gentoo')\ngentoo <- na.omit(gentoo)\nfit_gentoo <- brm(data = gentoo, bill_length_mm ~ 1 + body_mass_g)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 14.0.3 (clang-1403.0.22.14.1)’\nusing SDK: ‘MacOSX13.3.sdk’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include <cmath>\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.045 seconds (Warm-up)\nChain 1:                0.027 seconds (Sampling)\nChain 1:                0.072 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.036 seconds (Warm-up)\nChain 2:                0.027 seconds (Sampling)\nChain 2:                0.063 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 3:                0.029 seconds (Sampling)\nChain 3:                0.058 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.036 seconds (Warm-up)\nChain 4:                0.027 seconds (Sampling)\nChain 4:                0.063 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit_gentoo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: gentoo (Number of observations: 119) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      26.56      2.19    22.14    30.93 1.00     4268     2754\nbody_mass_g     0.00      0.00     0.00     0.00 1.00     4456     2833\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.34      0.16     2.06     2.68 1.01     1541     1577\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit_gentoo) %>% \n  rename(`beta[0]` = b_Intercept,\n         `beta[1]` = b_body_mass_g) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit_gentoo\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(fit_gentoo) %>% \n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnd_gentoo <- tibble(body_mass_g = seq(\n  from = min(gentoo$body_mass_g),\n  to = max(gentoo$body_mass_g),\n  length.out = 100))\n\nfitted(fit_gentoo, \n       newdata = nd_gentoo,\n       probs = c(.025, .975, .25, .75)) %>% \n  data.frame() %>% \n  bind_cols(nd_gentoo) %>% \n  ggplot(aes(x = body_mass_g)) +\n  # 95% range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 50% range\n  geom_ribbon(aes(ymin = Q25, ymax = Q75),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = gentoo,\n             aes(y = bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit_gentoo, \n       newdata = nd_gentoo,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd_gentoo, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value, fill = after_stat(.width))) +\n  # make more ribbons\n  stat_lineribbon(.width = ppoints(50)) +\n  scale_fill_distiller(limits = 0:1) +\n  coord_cartesian(ylim = range(gentoo$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nf_gentoo <- fitted(fit_gentoo, newdata = nd_gentoo) %>% \n  data.frame() %>% \n  bind_cols(nd_gentoo) \n\npredict(fit_gentoo, newdata = nd_gentoo) %>% \n  data.frame() %>% \n  bind_cols(nd_gentoo) %>% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% posterior-predictive range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 95% conditional mean range\n  geom_ribbon(data = f_gentoo,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # posterior mean of the conditional mean\n  geom_line(data = f_gentoo,\n            aes(y = Estimate)) +\n  # original data\n  geom_point(data = gentoo,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(gentoo$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\n# walk this code through\nas_draws_df(fit_gentoo) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 6) %>% \n  expand_grid(gentoo %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ .draw, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\nas_draws_df(fit_gentoo) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 50) %>%  # increase the number of random draws\n  expand_grid(gentoo %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = bill_length_mm, group = .draw)) + \n  geom_density(size = 1/4, color = alpha(\"black\", 1/2)) +\n  coord_cartesian(xlim = range(gentoo$bill_length_mm) + c(-5, 0))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\n\n## References\n\nKruschke, J. K. (2015). *Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan*. Academic Press. <https://sites.google.com/site/doingbayesiandataanalysis/>\n\n## Session information\n",
    "supporting": [
      "Bayes_Lab_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}