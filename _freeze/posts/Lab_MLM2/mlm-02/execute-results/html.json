{
  "hash": "c69fbaee4dcbf156a898ad13448bc8ae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multilevel Modeling (with R) Part 2\"\nsubtitle: \"Princeton University\"\nauthor: \"Suyog Chandramouli (adapted by materials from Jason Geller)\"\ndate: 'Updated:2025-03-31'\nfooter: \"PSY 504: Advanced Statistics\"\nformat: \n  revealjs:\n    theme: white\t\n    css: slide-style.css\n    multiplex: true\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    fontsize: \"25pt\"\nwebr:\n  packages: [\"tidyverse\", \"easystats\", \"broom\", \"kableExtra\", \"interactions\", \"emmeans\", \"lme4\",\"lmertest\",  \"ggeffects\"]\nfilters:\n  - webr\nexecute:\n  freeze: auto\n  echo: true\n  message: false\n  warning: false\n  fig-align: center\n  fig-width: 16\n  fig-height: 12\n  editor_options: \n  chunk_output_type: inline\n  code-overflow: wrap\n  html:\n    code-fold: true\n    code-tools: true\n---\n\n\n\n## Multilevel models\n\n-   When to use them:\n\n    -   Nested designs\n\n    -   Repeated measures\n\n    -   Longitudinal data\n\n    -   Complex designs\n\n-   Why use them:\n\n    -   Captures variance occurring between groups and within groups\n\n-   What they are:\n\n    -   Linear model with extra residuals\n\n## Today\n\n-   Everything you need to know to run and report a MLM\n\n    -   Organizing data for MLM analysis\n    -   Estimation\n    -   Fit and interpret multilevel models\n    -   Visualization\n    -   Effect size\n    -   Reporting\n    -   Power\n\n## Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # data wrangling\nlibrary(knitr) # nice tables\nlibrary(lme4) # fit mixed models\nlibrary(lmerTest) # mixed models\nlibrary(broom.mixed) # tidy output of mixed models\nlibrary(afex) # fit mixed models for lrt test\nlibrary(emmeans) # marginal means\nlibrary(ggeffects) # marginal means\nlibrary(ggrain) # rain plots\nlibrary(easystats) # nice ecosystem of packages\n\noptions(scipen=999) # get rid of sci notation\n```\n:::\n\n\n\n-   Find the .qmd document here to follow along: <https://github.com/suyoghc/PSY-504_Spring-2025/blob/main/Multilevel%20Modeling/mlm-02.qmd>\n\n## Today's data\n\n-   What did you say?\n\n    -   Ps (*N* = 31) listened to *both* clear (NS) and 6 channel vocoded speech (V6)\n        -   (https://www.mrc-cbu.cam.ac.uk/personal/matt.davis/vocode/a1_6.wav)\n            -   Fixed factor: ?\n            -   Random factor: ?\n\n## Today's data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\neye  <- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Multilevel%20Modeling/data/vocoded_pupil.csv\") # data for class\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](mlm-02_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=1536}\n:::\n:::\n\n\n\n## Data organization\n\n-   Data Structure\n\n    -   MLM analysis (in R) requires data in long format\n\n![](images/pivot_longer.png){fig-align=\"center\"}\n\n## Data organization\n\n-   Level 1: trial\n\n-   Level 2: subject\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|subject | trial|vocoded | mean_pupil|\n|:-------|-----:|:-------|----------:|\n|EYE15   |     3|V6      |  0.0839555|\n|EYE15   |     4|V6      |  0.0141083|\n|EYE15   |     5|V6      |  0.0224967|\n|EYE15   |     6|V6      |  0.0007424|\n|EYE15   |     7|V6      |  0.0242540|\n|EYE15   |     8|V6      |  0.0267617|\n\n\n:::\n:::\n\n\n\n## Centering\n\n::: columns\n::: {.column width=\"50%\"}\n-   In a single-level regression, centering ensures that the zero value for each predictor is meaningful before running the model\n\n-   In MLM, if you have specific questions about within, between, and contextual effects, you need to center!\n:::\n\n::: {.column width=\"50%\"}\n![](images/centering_mlm.png)\n\n![](){fig-align=\"center\"}\n:::\n:::\n\n## Group- vs. Grand-Mean Centering\n\n-   Grand-mean centering: $x_{ij} - x$\n\n    -   Variable represents each observation's deviation from everyone's norm, regardless of group\n\n-   Group-mean centering: $x_{ij} - x_j$\n\n    -   Variable represents each observation's deviation from their group's norm (removes group effect)\n\n## Group- vs. Grand-Mean Centering\n\n::: columns\n::: {.column width=\"50%\"}\n-   Level 1 predictors\n\n    -   Grand-mean centering\n\n        -   **Include means of level 2**\n            -   Allows us to directly test within-group effect\n            -   Coefficient associated with the Level 2 group mean represents **contextual effect**\n:::\n\n::: {.column width=\"50%\"}\n-   Group-mean centering\n\n    -   Level 1 coefficient will always be with within-group effect, regardless of whether the group means are included at Level 2 or not\n    -   If level 2 means included, coefficient represents the between-groups effect\n:::\n:::\n\n::: callout-note\nCan apply to categorical predictors as well (see Yaremych, Preacher, & Hedeker, 2023)\n:::\n\n## Centering in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how to group mean center \neye_centered <- eye %>% \n  # Grand mean centering (CMC)\n  mutate(iv.gmc = mean_pupil - mean(mean_pupil)) %>%\n  # group mean centering (more generally, centering within cluster)\n  group_by(subject) %>% \n  mutate(iv.cm = mean(mean_pupil),\n         iv.cwc = mean_pupil - iv.cm)\n```\n:::\n\n\n\n# Model Estimation\n\n## Maximum Likelihood\n\n<br>\n\n<br>\n\n<br>\n\n-   In MLM we try to maximize the likelihood of the data\n\n    -   No OLS!\n\n## Probability vs. Likelihood\n\n-   Probability\n\n> If I assume a distribution with certain parameters (fixed), what is the probability I see a particular value in the data?\n\n::: columns\n::: {.column width=\"50%\"}\n-   Pr‚Å°(ùë¶\\>0‚îÇùúá=0,ùúé=1)=.50\n\n-   Pr‚Å°(‚àí1\\<ùë¶\\<1‚îÇùúá=0,ùúé=1)=.68\n\n-   Pr‚Å°(0\\<ùë¶\\<1‚îÇùúá=0,ùúé=1)=.34\n\n-   Pr‚Å°(ùë¶\\>2‚îÇùúá=0,ùúé=1)=.02\n:::\n\n::: {.column width=\"50%\"}\n![](images/normal.png){fig-align=\"center\"}\n:::\n:::\n\n## Likelihood\n\n::: columns\n::: {.column width=\"50%\"}\n-   $L(ùúá,ùúé‚îÇùë•)$\n\n-   Holding a sample of data constant, which parameter values are more likely?\n\n    -   Which values have higher likelihood?\n\n    *Here data is fixed and distribution can change*\n:::\n\n::: {.column width=\"50%\"}\n![](images/likelihood-2.png){fig-align=\"center\"}\n:::\n:::\n\n## Likelihood\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/like1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Likelihood\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/like2.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Likelihood\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/like4.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Likelihood\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/like5.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Likelihood\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/like6.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Likelihood\n\nInteractive: Understanding Maximum Likelihood Estimation: <https://rpsychologist.com/likelihood/>\n\n## Log likelihood\n\n-   With large samples, likelihood values ‚Ñí(ùúá,ùúé‚îÇùë•) get very small very fast\n\n    -   To make them easier to work with, we usually work with the log-likelihood\n        -   Measure of how well the model fits the data\n        -   Higher values of $\\log L$ are better\n\n-   Deviance = $-2logL$\n\n    -   $-2logL$ follows a $\\chi^2$ distribution with $n (\\text{sample size}) - p (\\text{paramters}) - 1$ degrees of freedom\n\n## $\\chi^2$ distribution\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](mlm-02_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=1536}\n:::\n:::\n\n\n\n## Comparing nested models\n\n-   Suppose there are two models:\n\n    -   Reduced model includes predictors $x_1, \\ldots, x_q$\n    -   Full model includes predictors $x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p$\n\n-   We want to test the hypotheses:\n\n    -   $H_0$: smaller model is better\n\n    -   $H_1$: Larger model is better\n\n-   To do so, we will use the drop-in-deviance test (also known as the nested likelihood ratio test)\n\n## Drop-In-Deviance Test\n\n-   Hypotheses:\n\n    -   $H_0$: smaller model is better\n\n    -   $H_1$: Larger model is better\n\n-   Test Statistic: $$G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})$$\n\n-   P-value: $P(\\chi^2 > G)$:\n\n    -   Calculated using a $\\chi^2$ distribution\n    -   df = $df_1$ - $df_2$\n\n## Testing deviance\n\n-   We can use the `anova` function to conduct this test\n\n    -   Add test = \"Chisq\" to conduct the drop-in-deviance test\n\n-   I like `test_likelihoodratio` from `easystats`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- lmer(mean_pupil ~ 1 + (1|subject), data = eye, REML = FALSE)\nmodel2 <- lmer(mean_pupil ~ vocoded + (1|subject), data = eye, REML = FALSE)\nanova(model1, model2, test=\"chisq\")\n\n# test using easystats function\n\ntest_likelihoodratio(model1, model2)\n```\n:::\n\n\n\n## Model fitting: ML or REML?\n\n-   Two flavors of maximum likelihood\n\n    -   Maximum Likelihood (ML or FIML)\n\n        -   Jointly estimate the fixed effects and variance components using all the sample data\n\n        -   Can be used to draw conclusions about fixed and random effects\n\n        -   Issue:\n\n            -   Results are biased because fixed effects are estimated without error\n\n## Model fitting: ML or REML\n\n-   Restricted Maximum Likelihood (REML)\n\n    -   Estimates the variance components using the sample residuals not the sample data\n\n    -   It is conditional on the fixed effects, so it accounts for uncertainty in fixed effects estimates\n\n        -   This results in unbiased estimates of variance components\n        -   Associated with error/penalty\n\n## Model fitting: ML or REML?\n\n-   Research has not determined one method absolutely superior to the other\n\n-   **REML** (`REML = TRUE`; default in `lmer`) is preferable when:\n\n    -   The number of parameters is large\n\n    -   Primary objective is to obtain relaible estimates of the variance parameters\n\n    -   For REML, likelihood ratio tests can only be used to draw conclusions about variance components\n\n-   **ML** (`REML = FALSE`) <u>must</u> be used if you want to compare nested fixed effects models using a likelihood ratio test (e.g., a drop-in-deviance test)\n\n## ML or REML?\n\n-   What would we use if we wanted to compare the below models?\nUse ML because it is comparing fixed effect (accounting for interaction in y)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx= lmer(DV ~ IV1 + IV2 + (1|ID))\n\ny= lmer(DV ~ IV1*IV2 + (1|ID))\n```\n:::\n\n\n\n## ML or REML?\n\n-   What would we use if we wanted to compare the below models?\nUse REML since it is comparing random effect to see if the effect of IV2 (slope) differs across subjects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = lmer(DV ~ IV1 + IV2 + (1+IV2|ID))\n\ny = lmer(DV ~ IV1+ IV2 + (1|ID))\n```\n:::\n\n\n\n# Fitting and Interpreting Models\n\n## Modeling approach\n\n-   Forward/backward approach\n\n```{webr-r}\n\n```\n\n-   `Keep it maximal`[^1]\n\n    -   Whatever can vary, should vary\n\n        -   **Decreases Type 1 error**\n\n[^1]: Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 10.1016/j.jml.2012.11.001. https://doi.org/10.1016/j.jml.2012.11.001\n\n## Modeling approach\n\n-   Full (maximal) model\n\n    -   Only when there is convergence issues should you remove terms\n        -   if non-convergence (pay attention to warning messages in summary output!):\n            -   Try different optimizer (`afex::all_fit()`)\n                -   Sort out random effects\n                    -   Remove correlations between slopes and intercepts\n                    -   Random slopes\n                    -   Random Intercepts\n                -   Sort out fixed effects (e.g., interaction)\n                -   Once you arrive at the final model present it using REML estimation\n\n## Modeling approach\n\n-   If your model is singular (check output!!!!)\n\n    -   Variance might be close to 0\n    -   Perfect correlations (1 or -1)\n\n-   Drop the parameter!\n\n## Modeling approach\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read.csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Multilevel%20Modeling/data/heck2011.csv\")\n\nsummary(lmer(math~ses + (1+ses|schcode), data=data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: math ~ ses + (1 + ses | schcode)\n   Data: data\n\nREML criterion at convergence: 48190.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8578 -0.5553  0.1290  0.6437  5.7098 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schcode  (Intercept)  3.2042  1.7900        \n          ses          0.7794  0.8828   -1.00\n Residual             62.5855  7.9111        \nNumber of obs: 6871, groups:  schcode, 419\n\nFixed effects:\n             Estimate Std. Error        df t value            Pr(>|t|)    \n(Intercept)   57.6959     0.1315  378.6378  438.78 <0.0000000000000002 ***\nses            3.9602     0.1408 1450.7730   28.12 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n    (Intr)\nses -0.284\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n```\n\n\n:::\n:::\n\n\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmer(math~ses + (1+ses||schcode), data=data) # removes correlation() with double pipes. Does not work with categorical variables\n```\n:::\n\n\n\n## Null model (unconditional means)\n\nGet ICC\n\n-   ICC is a standardized way of expressing how much variance is due to clustering/group\n    -   Ranges from 0-1\n-   Can also be interpreted as correlation among observations within cluster/group!\n-   If ICC is sufficiently low (i.e., $\\rho$ \\< .1), then you don't have to use MLM! *BUT YOU PROBABLY SHOULD üôÇ*\n\n## Null model (unconditional means)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4) # pop linear modeling package\n\nnull_model <- lmer(mean_pupil ~ (1|subject), data = eye, REML=TRUE)\n\nsummary(null_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ (1 | subject)\n   Data: eye\n\nREML criterion at convergence: -19811.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.1411 -0.5530 -0.0463  0.4822 10.8130 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n subject  (Intercept) 0.0001303 0.01142 \n Residual             0.0016840 0.04104 \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(>|t|)  \n(Intercept)  0.005227   0.002124 29.457784   2.461   0.0199 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n## Calculating ICC\n\n-   Run baseline (null) model\n\n-   Get intercept variance and residual variance\n\n$$\\mathrm{ICC}=\\frac{\\text { between-group variability }}{\\text { between-group variability+within-group variability}}$$\n\n$$\nICC=\\frac{\\operatorname{Var}\\left(u_{0 j}\\right)}{\\operatorname{Var}\\left(u_{0 j}\\right)+\\operatorname{Var}\\left(e_{i j}\\right)}=\\frac{\\tau_{00}}{\\tau_{00}+\\sigma^{2}}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# easystats \n#adjusted icc just random effects\n#unadjusted fixed effects taken into account\nperformance::icc(null_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.072\n  Unadjusted ICC: 0.072\n```\n\n\n:::\n:::\n\n\n\n## Maximal model: Fixed effect random intercepts (subject) and slopes (vocoded) model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_model <- lmer(mean_pupil ~vocoded +(1+vocoded|subject), data = eye)\n\nsummary(max_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(>|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306\n```\n\n\n:::\n:::\n\n\n\n## Fixed effects\n\n-   Interpretation same as lm\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#grab the fixed effects\nsummary(max_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(>|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306\n```\n\n\n:::\n:::\n\n\n\n## Degrees of freedom and p-values\n\n-   Degrees of freedom (denominator) and *p*-values can be assessed with several methods:\n\n    -   ***Satterthwaite*** (default when install `lmerTest` and then run `lmer`)\n\n    -   Asymptotic (Inf) (**default** behavior lme4)\n\n    -   Kenward-Rogers\n\n## Random effects/variance components\n\n-   Tells us how much variability there is around the fixed intercept/slope\n\n    -   How much does the average pupil size change between participants\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(max_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(>|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306\n```\n\n\n:::\n:::\n\n\n\n## Random effects/variance components\n\n-   Correlation between random intercepts and slopes\n\n    -   Negative correlation\n\n        -   Higher intercept (for normal speech) less of effect (lower slope)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter1 |  Parameter2 |     r |        95% CI | t(29) |     p\n----------------------------------------------------------------\nvocodedV6  | (Intercept) | -0.10 | [-0.44, 0.26] | -0.57 | 0.576\n\nObservations: 31\n```\n\n\n:::\n:::\n\n\n\n## Visualize Random Effects\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# use easystats to grab group variance\nrandom <- estimate_grouplevel(max_model)\n\nplot(random) + theme_lucid()\n```\n\n::: {.cell-output-display}\n![](mlm-02_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=1536}\n:::\n:::\n\n\n\n## Model comparisons\n\n-   Can compare models using `anova` function or `test_likelihoodratio` from `easystats`\n\n    -   *Will be refit using ML if interested in fixed effects*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# you try\nanova(null_model, max_model, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: eye\nModels:\nnull_model: mean_pupil ~ (1 | subject)\nmax_model: mean_pupil ~ vocoded + (1 + vocoded | subject)\n           npar    AIC    BIC logLik deviance  Chisq Df Pr(>Chisq)   \nnull_model    3 -19816 -19796 9911.1   -19822                        \nmax_model     6 -19823 -19784 9917.7   -19835 13.269  3    0.00409 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n# AIC/BIC\n\n-   LRT requires nested models\n\n## AIC\n\n-   AIC:\n\n$$\nD + 2p\n$$\n\n-   where d = deviance and p = \\# of parameters in model\n\n-   Can compare AICs[^2]:\n\n    $$\n     \\Delta_i = AIC_{i} - AIC_{min}\n    $$\n\n-   Less than 2: More parsimonious model is preferred\n\n-   Between 4 and 7: some evidence for lower AIC model\n\n-   Greater than 10,: strong evidence for lower AIC\n\n[^2]: BURNHAM, ANDERSON, & HUYVAERT (2011)\n\n## BIC\n\n-   BIC:\n\n$$\nD + ln(n)*p\n$$\n\n-   where d = deviance, p = \\# of parameters in model, n = sample size\n\n-   Change in BIC:\n\n    -   $\\Delta{BIC}$ \\<= 2 (No difference)\n\n    -   $\\Delta{BIC}$ \\> 3 (evidence for smaller BIC model)\n\n## AIC/BIC\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::model_performance(max_model) %>% # easystats\n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|       AIC|      AICc|       BIC| R2_conditional| R2_marginal|      ICC|      RMSE|     Sigma|\n|---------:|---------:|---------:|--------------:|-----------:|--------:|---------:|---------:|\n| -19801.66| -19801.65| -19761.87|      0.0773469|   0.0013442| 0.076105| 0.0407697| 0.0409264|\n\n\n:::\n:::\n\n\n\n## Hypothesis testing\n\n-   Multiple options\n\n    1.  t/F tests with approximate degrees of freedom (Kenward-Rogers or Satterwaithe)\n    2.  Parametric bootstrap\n    3.  **Likelihood ratio test (LRT)**\n\n    -   Can be interpreted as main effects and interactions\n    -   Use `afex` package to do that\n\n## Hypothesis testing - `afex`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(afex) # load afex in \n\nm <- mixed(mean_pupil ~ 1 + vocoded +  (1+vocoded|subject), data =eye, method = \"LRT\") # fit lmer using afex\n\nnice(m) %>%\n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|Effect  | df|Chisq  |p.value |\n|:-------|--:|:------|:-------|\n|vocoded |  1|4.47 * |.034    |\n\n\n:::\n:::\n\n\n\n## Using `emmeans`\n\n-   Get means and contrasts\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(emmeans) # get marginal means \n\nemmeans(max_model, specs = \"vocoded\") %>% \n  kable() # grabs means/SEs for each level of vocode \n```\n\n::: {.cell-output-display}\n\n\n|vocoded |    emmean|        SE|  df|  asymp.LCL| asymp.UCL|\n|:-------|---------:|---------:|---:|----------:|---------:|\n|NS      | 0.0036427| 0.0022348| Inf| -0.0007374| 0.0080229|\n|V6      | 0.0067668| 0.0022618| Inf|  0.0023337| 0.0111999|\n\n\n:::\n\n```{.r .cell-code}\npairs(emmeans(max_model, specs = \"vocoded\")) %>%\n  confint() %>%\n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|contrast |   estimate|        SE|  df|  asymp.LCL|  asymp.UCL|\n|:--------|----------:|---------:|---:|----------:|----------:|\n|NS - V6  | -0.0031241| 0.0014532| Inf| -0.0059723| -0.0002759|\n\n\n:::\n\n```{.r .cell-code}\n# use this to get pariwise compairsons between levels of factors\n```\n:::\n\n\n\n# Assumptions\n\n## Check assumptions\n\n::: columns\n::: {.column width=\"50%\"}\n-   Linearity\n\n-   Normality\n\n    -   Level 1 residuals are normally distributed around zero\n\n    -   Level 2 residuals are multivariate-normal with a mean of zero\n\n-   Homoskedacticity\n\n    -   Level 1/Level 2 predictors and residuals are homoskedastic\n:::\n\n::: {.column width=\"50%\"}\n-   Collinearity\n\n-   Outliers\n:::\n:::\n\n## Assumptions\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(easystats) # performance package\n\ncheck_model(max_model)\n```\n\n::: {.cell-output-display}\n![](mlm-02_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=1536}\n:::\n:::\n\n\n\n## Visualization\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mlm-02_files/figure-revealjs/unnamed-chunk-29-1.png){width=1536}\n:::\n:::\n\n\n\n## `ggeffects`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggemmeans(max_model, terms=c(\"vocoded\")) %>% plot()\n```\n\n::: {.cell-output-display}\n![](mlm-02_files/figure-revealjs/unnamed-chunk-30-1.png){fig-align='center' width=1536}\n:::\n:::\n\n\n\n## Effect size\n\n-   Report pseudo-$R^2$ for marginal (fixed) and conditional model (full model) (Nakagawa et al. 2017)\n\n$$\nR^2_{LMM(c)} = \\frac{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random}}{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random} + \\sigma_e^2\\text{residual}}\n$$\n\n$$\nR^2_{\\text{LMM}(m)} = \\frac{\\sigma_f^2\\text{fixed}}{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random} + \\sigma_e^2\\text{residual}}\n$$\n\n-   Report semi-partial $R^2$ for each predictor variable\n    -   $R^2_\\beta$\n        -   `partR2` package in R does this for you\n\n## Effect size\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#get r2 for model with performance from easystats\n\nperformance::r2(max_model) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# R2 for Mixed Models\n\n  Conditional R2: 0.077\n     Marginal R2: 0.001\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# get semi-part\nlibrary(partR2) # does NOT work\n# does not work with random slopes for some reason :/\n#R2_3 <- partR2(max_model,data=eye, \n#  partvars = c(\"vocoded\"),\n#  R2_type = \"marginal\", nboot = 10, CI = 0.95\n#)\nr2_values <- performance::r2(max_model)\nprint(r2_values)\n```\n:::\n\n\n\n## Effect size\n\n-   Cohen's $d$ for treatment effects/categorical predictions[^3]\n\n[^3]: Brysbaert, M., & Debeer, D. (2023, September 12). How to run linear mixed effects analysis for pairwise comparisons? A tutorial and a proposal for the calculation of standardized effect sizes. <https://doi.org/10.31234/osf.io/esnku>\n\n$$\nd = \\frac{\\text{Effect}}{\\sqrt{\\sigma^2_\\text{Intercept} + \\sigma^2_\\text{slope} + \\sigma^2_\\text{residual}}}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(max_model,~ vocoded) %>% \n eff_size(.,sigma=.04, edf=30) # need to calcuate sigma and add dfs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast effect.size     SE  df asymp.LCL asymp.UCL\n NS - V6      -0.0781 0.0377 Inf    -0.152  -0.00421\n\nsigma used for effect sizes: 0.04 \nDegrees-of-freedom method: inherited from asymptotic when re-gridding \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\n# Reporting Results\n\n## Describing a MLM analysis - Structure\n\n-   What was the nested data structure (e.g., how many levels; what were the units at each level?)\n\n    ‚Ä¢ How many units were in each level, on average?\n\n    ‚Ä¢ What was the range of the number of lower-level units in each group/cluster?\n    Two-level hierachical sturcture with level 1 -> trial (mean: 181 trials) and level 2 -> subject (31 people).\n\n## Describing a MLM analysis - Model\n\n::: columns\n::: {.column width=\"50%\"}\n-   What equation can best represent your model?\nLevel 1 - mean_pupil_ij = Œ≤0j + Œ≤1j(vocoded_ij) + eij\nLevel 2 - Œ≤0j = Œ≥00 + u0j\nŒ≤1j = Œ≥10 + u1j\nCombined: mean_pupil_ij = Œ≥00 + Œ≥10(vocoded_ij) + u0j + u1j(vocoded_ij) + eij\n-   What estimation method was used (e.g., ML, REML)?\nREML was used for the final model.\n-   If there were convergence issues, how was this addressed?\nIf there were, removing terms should help\n-   What software (and version) was used (when using R, what packages as well)?\nR 4.4.3; lme4 1.1.36; lmerTest 3.1.3\n\n::: {.column width=\"50%\"}\n-   If degrees of freedom were used, what kind?\nSatterthwaite\n-   What type of models were estimated (i.e., unconditional, random intercept, random slope, max)?\nunconditional means model (null model with only random intercepts) and a max model with fixed effects for vocoded condition and random intercepts and slopes for subjects\n-   What variables were centered and what kind of centering was used?\nNo centering was used? if used, subtract the mean\n-   What model assumptions were checked and what were the results?\nnormality, homogeneity of variance, and linearity\n:::\n:::\n\n## Describing a MLM analysis - Results\n\n::: columns\n::: {.column width=\"50%\"}\n-   What was the ICC of the outcome variable?\n0.072\n-   Are fixed effects and variance components reported?\nfixed\nEstimate  Std. Error       df  t value   Pr(>|t|)\n(Intercept) 0.003642709 0.002234812 28.85229 1.629985 0.11397535\nvocodedV6   0.003124078 0.001453186 30.47199 2.149813 0.03962519\n\nrandom/variance components\nGroups   Name        Std.Dev.  Corr  \n subject  (Intercept) 0.0116584       \n          vocodedV6   0.0053071 -0.195\n Residual             0.0409264  \n-   What inferential statistics were used (e.g., t-statistics, LRTs)?\nLRT and t-tests\n-   How precise were the results (report the standard errors and/or confidence intervals)?\nSE for Intercept: 0.00223481165112141\nSE for Vocoded Effect: 0.00145318603511761\n:::\n\n::: {.column width=\"50%\"}\n-   Were model comparisons performed (e.g., AIC, BIC, if using an LRT,report the œá2, degrees of freedom, and p value)?\nLRT\nrefitting model(s) with ML (instead of REML)\nData: eye\nModels:\nnull_model: mean_pupil ~ (1 | subject)\nmax_model: mean_pupil ~ vocoded + (1 + vocoded | subject)\n           npar    AIC    BIC logLik deviance  Chisq Df Pr(>Chisq)   \nnull_model    3 -19816 -19796 9911.1   -19822                        \nmax_model     6 -19823 -19784 9917.7   -19835 13.269  3    0.00409 **\n\n\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n-   Were effect sizes reported for overall model and individual predictors (e.g., Cohen‚Äôs d, $R^2$ )?\n contrast effect.size     SE  df asymp.LCL asymp.UCL\n NS - V6      -0.0781 0.0377 Inf    -0.152  -0.00421\n:::\n:::\n\n## Write-up\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::report(max_model) # easystats report function\n```\n:::\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict mean_pupil with vocoded (formula: mean_pupil ~ vocoded). The model included vocoded as random effects (formula: ~1 + vocoded | subject). The model's total explanatory power is weak (conditional R2 = 0.08) and the part related to the fixed effects alone (marginal R2) is of 1.34e-03. The model's intercept, corresponding to vocoded = NS, is at 3.64e-03 (95% CI [-7.38e-04, 8.02e-03], t(5603) = 1.63, p = 0.103). Within this model:\n\n  - The effect of vocoded [V6] is statistically significant and positive (beta = 3.12e-03, 95% CI [2.75e-04, 5.97e-03], t(5603) = 2.15, p = 0.032; Std. beta = 0.07, 95% CI [6.49e-03, 0.14])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation.\n\n## Table\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelsummary::modelsummary(list(\"max model\" = max_model), output=\"html\") # modelsummary package\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_wkencyp79qr7wn4uoc8j(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_wkencyp79qr7wn4uoc8j\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_wkencyp79qr7wn4uoc8j');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_wkencyp79qr7wn4uoc8j(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_wkencyp79qr7wn4uoc8j\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 15, j: 1 },  ], css_id: 'tinytable_css_4ap2cvu7d4dydjtg1rqp',}, \n          { positions: [ { i: 8, j: 1 },  ], css_id: 'tinytable_css_26e38zd1t0v20x9irjs8',}, \n          { positions: [ { i: 1, j: 1 }, { i: 2, j: 1 }, { i: 3, j: 1 }, { i: 4, j: 1 }, { i: 5, j: 1 }, { i: 6, j: 1 }, { i: 7, j: 1 }, { i: 12, j: 1 }, { i: 9, j: 1 }, { i: 10, j: 1 }, { i: 11, j: 1 }, { i: 13, j: 1 }, { i: 14, j: 1 },  ], css_id: 'tinytable_css_9gdi41o21tnob1yvftkr',}, \n          { positions: [ { i: 0, j: 1 },  ], css_id: 'tinytable_css_iv23fkat10nku65qi36o',}, \n          { positions: [ { i: 15, j: 0 },  ], css_id: 'tinytable_css_vkwgi0b12kgjv6pw3pyt',}, \n          { positions: [ { i: 8, j: 0 },  ], css_id: 'tinytable_css_ik60loahdhqdyp2ro2dg',}, \n          { positions: [ { i: 1, j: 0 }, { i: 2, j: 0 }, { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 5, j: 0 }, { i: 6, j: 0 }, { i: 7, j: 0 }, { i: 12, j: 0 }, { i: 9, j: 0 }, { i: 10, j: 0 }, { i: 11, j: 0 }, { i: 13, j: 0 }, { i: 14, j: 0 },  ], css_id: 'tinytable_css_dokzenywkhnl04xe81ye',}, \n          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_my7xdzof2u5wy1v0t7aj',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_wkencyp79qr7wn4uoc8j(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_4ap2cvu7d4dydjtg1rqp, .table th.tinytable_css_4ap2cvu7d4dydjtg1rqp { text-align: center; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_26e38zd1t0v20x9irjs8, .table th.tinytable_css_26e38zd1t0v20x9irjs8 { text-align: center; border-bottom: solid black 0.05em; }\n      .table td.tinytable_css_9gdi41o21tnob1yvftkr, .table th.tinytable_css_9gdi41o21tnob1yvftkr { text-align: center; }\n      .table td.tinytable_css_iv23fkat10nku65qi36o, .table th.tinytable_css_iv23fkat10nku65qi36o { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_vkwgi0b12kgjv6pw3pyt, .table th.tinytable_css_vkwgi0b12kgjv6pw3pyt { text-align: left; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_ik60loahdhqdyp2ro2dg, .table th.tinytable_css_ik60loahdhqdyp2ro2dg { text-align: left; border-bottom: solid black 0.05em; }\n      .table td.tinytable_css_dokzenywkhnl04xe81ye, .table th.tinytable_css_dokzenywkhnl04xe81ye { text-align: left; }\n      .table td.tinytable_css_my7xdzof2u5wy1v0t7aj, .table th.tinytable_css_my7xdzof2u5wy1v0t7aj { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_wkencyp79qr7wn4uoc8j\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">max model</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>(Intercept)</td>\n                  <td>0.004</td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.002)</td>\n                </tr>\n                <tr>\n                  <td>vocodedV6</td>\n                  <td>0.003</td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.001)</td>\n                </tr>\n                <tr>\n                  <td>SD (Intercept subject)</td>\n                  <td>0.012</td>\n                </tr>\n                <tr>\n                  <td>SD (vocodedV6 subject)</td>\n                  <td>0.005</td>\n                </tr>\n                <tr>\n                  <td>Cor (Intercept~vocodedV6 subject)</td>\n                  <td>‚àí0.195</td>\n                </tr>\n                <tr>\n                  <td>SD (Observations)</td>\n                  <td>0.041</td>\n                </tr>\n                <tr>\n                  <td>Num.Obs.</td>\n                  <td>5609</td>\n                </tr>\n                <tr>\n                  <td>R2 Marg.</td>\n                  <td>0.001</td>\n                </tr>\n                <tr>\n                  <td>R2 Cond.</td>\n                  <td>0.077</td>\n                </tr>\n                <tr>\n                  <td>AIC</td>\n                  <td>‚àí19801.7</td>\n                </tr>\n                <tr>\n                  <td>BIC</td>\n                  <td>‚àí19761.9</td>\n                </tr>\n                <tr>\n                  <td>ICC</td>\n                  <td>0.1</td>\n                </tr>\n                <tr>\n                  <td>RMSE</td>\n                  <td>0.04</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n## Power\n\n-   Simulation-based power analyses\n\n    -   Simulate new data\n\n        -   `faux` (<https://debruine.github.io/faux/articles/sim_mixed.html>)\n\n    -   Use pilot data (what I would do)\n\n        -   `mixedpower`(https://link.springer.com/article/10.3758/s13428-021-01546-0)\n        -   `simr` (<https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504>)\n\n    ## \n",
    "supporting": [
      "mlm-02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}